Multi-Agent Financial ML Research Assistant
Project Overview
A sophisticated multi-agent system that answers complex questions about machine learning applications in finance using advanced retrieval techniques. The system orchestrates multiple specialized agents to provide comprehensive, well-cited responses from a curated database of financial ML research papers.
Core Architecture
Multi-Agent System

Query Analyzer Agent: Classifies questions, identifies complexity, breaks down multi-part queries
Retrieval Agent: Implements hybrid retrieval (dense + sparse) with hierarchical document understanding
Domain Expert Agent: Provides financial ML expertise, methodology analysis, and cross-paper insights
Orchestrator Agent: Coordinates workflow, manages agent communication, ensures response quality

Advanced RAG Pipeline

Hybrid Retrieval: Combines semantic similarity (SentenceTransformers) with keyword matching (BM25)
Hierarchical Chunking: Documents structured by paper → section → subsection → paragraph
Metadata Filtering: Filter by methodology, dataset, publication year, authors
Context Expansion: Intelligently includes parent/child sections for complete context

financial-ml-research-assistant/
├── src/
│   ├── ingestion/
│   │   ├── __init__.py
│   │   ├── pdf_extractor.py       # Extract text from PDFs
│   │   ├── paper_parser.py        # Parse paper structure
│   │   ├── chunker.py            # Hierarchical chunking strategies
│   │   └── metadata_extractor.py  # Extract paper metadata
│   ├── preprocessing/
│   │   ├── __init__.py
│   │   ├── text_cleaner.py       # Clean extracted text
│   │   ├── section_detector.py    # Detect paper sections
│   │   └── entity_extractor.py    # Extract financial/ML entities
│   ├── indexing/
│   │   ├── __init__.py
│   │   ├── embeddings_generator.py # Generate embeddings
│   │   ├── vector_store.py        # ChromaDB operations
│   │   └── bm25_indexer.py        # BM25 index creation
│   ├── agents/
│   │   ├── __init__.py
│   │   ├── base_agent.py          # Base agent class
│   │   ├── query_analyzer.py      
│   │   ├── retriever.py           
│   │   ├── domain_expert.py       
│   │   └── orchestrator.py        
│   ├── retrieval/
│   │   ├── __init__.py
│   │   ├── hybrid_search.py       
│   │   ├── reranker.py           
│   │   └── context_builder.py     
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── prompt_loader.py       # Load prompts from txt files
│   │   ├── citations.py          
│   │   └── metrics.py            
│   └── config.py                  
├── data/
│   ├── raw_papers/                # Your PDF papers
│   ├── processed_papers/          # JSON files with processed chunks
│   ├── chroma_db/                # Vector database
│   └── bm25_index/               # BM25 index files
├── prompts/                       # All prompts as .txt files
│   ├── query_analyzer/
│   │   ├── classify_query.txt
│   │   ├── extract_entities.txt
│   │   └── decompose_query.txt
│   ├── retriever/
│   │   ├── query_expansion.txt
│   │   └── rerank_chunks.txt
│   ├── domain_expert/
│   │   ├── analyze_methodology.txt
│   │   ├── compare_models.txt
│   │   └── synthesize_findings.txt
│   └── orchestrator/
│       ├── route_query.txt
│       ├── generate_response.txt
│       └── validate_response.txt
├── scripts/
│   ├── ingest_papers.py          # Run ingestion pipeline
│   ├── build_index.py            # Build search indices
│   └── evaluate_system.py        # Run evaluation
├── tests/
├── notebooks/
└── requirements.txt
Phase 1: Ingestion & Chunking Pipeline
1. PDF Extraction Module
python# src/ingestion/pdf_extractor.py
import pymupdf
import pdfplumber
from typing import Dict, List, Tuple
import re

class PDFExtractor:
    """
    Extracts text from financial ML research papers with structure preservation
    """
    
    def __init__(self, preserve_equations=True, extract_tables=True):
        self.preserve_equations = preserve_equations
        self.extract_tables = extract_tables
        
    def extract_from_pdf(self, pdf_path: str) -> Dict:
        """
        Extract text, tables, and metadata from PDF
        """
        extracted_data = {
            'raw_text': '',
            'pages': [],
            'tables': [],
            'metadata': {},
            'equations': []
        }
        
        # Try multiple extraction methods for robustness
        try:
            # Method 1: PyMuPDF for text and structure
            extracted_data.update(self._extract_with_pymupdf(pdf_path))
        except:
            # Fallback: pdfplumber
            extracted_data.update(self._extract_with_pdfplumber(pdf_path))
            
        # Extract paper metadata
        extracted_data['metadata'] = self._extract_metadata(extracted_data['raw_text'])
        
        return extracted_data
    
    def _extract_with_pymupdf(self, pdf_path: str) -> Dict:
        """
        Primary extraction using PyMuPDF
        """
        doc = pymupdf.open(pdf_path)
        pages = []
        
        for page_num, page in enumerate(doc):
            page_dict = {
                'page_num': page_num + 1,
                'text': page.get_text(),
                'blocks': page.get_text("blocks"),  # Preserve layout blocks
                'fonts': self._extract_fonts(page),  # For section detection
            }
            pages.append(page_dict)
            
        return {'pages': pages, 'raw_text': '\n'.join([p['text'] for p in pages])}
    
    def _extract_metadata(self, text: str) -> Dict:
        """
        Extract paper metadata from first pages
        """
        metadata = {}
        
        # Title (usually in first few lines with largest font)
        title_pattern = r'^(.+?)[\n\r]'
        title_match = re.search(title_pattern, text[:500], re.MULTILINE)
        if title_match:
            metadata['title'] = title_match.group(1).strip()
        
        # Authors (various formats)
        author_patterns = [
            r'(?:Authors?|by)[\s:]+([^\n]+)',
            r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+(?:,\s*[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)*)'
        ]
        
        # Abstract
        abstract_pattern = r'(?:Abstract|ABSTRACT)[\s:\n]+(.+?)(?:\n\n|Introduction|1\s*Introduction)'
        abstract_match = re.search(abstract_pattern, text[:3000], re.IGNORECASE | re.DOTALL)
        if abstract_match:
            metadata['abstract'] = abstract_match.group(1).strip()
        
        # Year
        year_pattern = r'(20[0-9]{2})'
        year_matches = re.findall(year_pattern, text[:1000])
        if year_matches:
            metadata['year'] = int(year_matches[0])
            
        return metadata
2. Paper Parser Module
python# src/ingestion/paper_parser.py
import re
from typing import List, Dict, Tuple

class PaperSectionParser:
    """
    Identifies and extracts paper sections with hierarchical structure
    """
    
    def __init__(self):
        # Common section patterns in financial ML papers
        self.section_patterns = {
            'main': [
                r'^\s*(\d+)\s*\.?\s*([A-Z][^\.]+)',  # 1. Introduction
                r'^\s*(I{1,3}|IV|V|VI|VII|VIII|IX|X)\s*\.\s*([A-Z][^\n]+)',  # Roman numerals
            ],
            'subsection': [
                r'^\s*(\d+\.\d+)\s*\.?\s*([A-Z][^\.]+)',  # 2.1 Data Description
                r'^\s*([A-Z])\.\s*([A-Z][^\n]+)',  # A. Methodology
            ],
            'subsubsection': [
                r'^\s*(\d+\.\d+\.\d+)\s*\.?\s*([^\n]+)',  # 2.1.1 Feature Engineering
            ]
        }
        
        # Financial ML specific sections to identify
        self.important_sections = [
            'abstract', 'introduction', 'literature review', 'related work',
            'methodology', 'model', 'data', 'dataset', 'features',
            'experiments', 'results', 'evaluation', 'performance',
            'conclusion', 'future work', 'references'
        ]
        
    def parse_structure(self, pages: List[Dict]) -> Dict:
        """
        Parse hierarchical structure of the paper
        """
        structure = {
            'sections': [],
            'hierarchy': {},
            'toc': []  # Table of contents
        }
        
        current_section = None
        current_subsection = None
        
        for page in pages:
            lines = page['text'].split('\n')
            
            for i, line in enumerate(lines):
                # Check for main sections
                section_match = self._match_section(line, 'main')
                if section_match:
                    section = {
                        'level': 1,
                        'number': section_match[0],
                        'title': section_match[1],
                        'page': page['page_num'],
                        'content': [],
                        'subsections': []
                    }
                    structure['sections'].append(section)
                    current_section = section
                    current_subsection = None
                    continue
                
                # Check for subsections
                subsection_match = self._match_section(line, 'subsection')
                if subsection_match and current_section:
                    subsection = {
                        'level': 2,
                        'number': subsection_match[0],
                        'title': subsection_match[1],
                        'page': page['page_num'],
                        'content': [],
                        'parent': current_section['title']
                    }
                    current_section['subsections'].append(subsection)
                    current_subsection = subsection
                    continue
                
                # Add content to appropriate section
                if line.strip():
                    if current_subsection:
                        current_subsection['content'].append(line)
                    elif current_section:
                        current_section['content'].append(line)
                        
        return structure
    
    def _match_section(self, line: str, level: str) -> Tuple:
        """
        Match section patterns and return (number, title)
        """
        for pattern in self.section_patterns[level]:
            match = re.match(pattern, line, re.MULTILINE)
            if match:
                return match.groups()
        return None
    
    def identify_key_sections(self, structure: Dict) -> Dict:
        """
        Identify and tag important sections for financial ML
        """
        tagged_sections = {}
        
        for section in structure['sections']:
            section_lower = section['title'].lower()
            
            for key_type in self.important_sections:
                if key_type in section_lower:
                    if key_type not in tagged_sections:
                        tagged_sections[key_type] = []
                    tagged_sections[key_type].append(section)
                    
            # Check subsections too
            for subsection in section.get('subsections', []):
                subsection_lower = subsection['title'].lower()
                if 'data' in subsection_lower or 'dataset' in subsection_lower:
                    tagged_sections.setdefault('data', []).append(subsection)
                elif 'model' in subsection_lower or 'architecture' in subsection_lower:
                    tagged_sections.setdefault('model', []).append(subsection)
                    
        return tagged_sections
3. Hierarchical Chunking Module
python# src/ingestion/chunker.py
from typing import List, Dict, Optional
import tiktoken
from dataclasses import dataclass

@dataclass
class ChunkConfig:
    """Configuration for different section types"""
    max_tokens: int
    overlap_tokens: int
    preserve_sentences: bool
    include_parent_context: bool

class HierarchicalChunker:
    """
    Creates hierarchical chunks with financial ML awareness
    """
    
    def __init__(self):
        self.encoder = tiktoken.encoding_for_model("gpt-4")
        
        # Different chunking strategies per section type
        self.chunk_configs = {
            'abstract': ChunkConfig(500, 50, True, False),
            'introduction': ChunkConfig(400, 50, True, False),
            'methodology': ChunkConfig(300, 75, True, True),  # Smaller, more overlap
            'model': ChunkConfig(300, 75, True, True),
            'data': ChunkConfig(350, 50, True, True),
            'results': ChunkConfig(400, 50, True, True),
            'conclusion': ChunkConfig(500, 50, True, False),
            'default': ChunkConfig(400, 50, True, True)
        }
        
    def chunk_paper(self, structure: Dict, paper_metadata: Dict) -> List[Dict]:
        """
        Create chunks from parsed paper structure
        """
        all_chunks = []
        chunk_id = 0
        
        for section_idx, section in enumerate(structure['sections']):
            # Determine chunk config
            section_type = self._identify_section_type(section['title'])
            config = self.chunk_configs.get(section_type, self.chunk_configs['default'])
            
            # Create section-level chunks
            section_text = '\n'.join(section['content'])
            section_chunks = self._create_chunks(
                text=section_text,
                config=config,
                metadata={
                    'paper_id': paper_metadata.get('title', 'unknown'),
                    'paper_metadata': paper_metadata,
                    'section': section['title'],
                    'section_number': section['number'],
                    'section_level': 1,
                    'section_type': section_type,
                    'page_start': section['page']
                }
            )
            
            # Add hierarchical references
            for chunk in section_chunks:
                chunk['chunk_id'] = f"chunk_{chunk_id}"
                chunk['hierarchy'] = {
                    'parent': None,
                    'children': [],
                    'level': 1,
                    'path': f"{section['number']}_{section['title']}"
                }
                all_chunks.append(chunk)
                chunk_id += 1
            
            # Process subsections
            for subsection in section.get('subsections', []):
                subsection_text = '\n'.join(subsection['content'])
                subsection_chunks = self._create_chunks(
                    text=subsection_text,
                    config=config,
                    metadata={
                        'paper_id': paper_metadata.get('title', 'unknown'),
                        'paper_metadata': paper_metadata,
                        'section': section['title'],
                        'subsection': subsection['title'],
                        'section_number': subsection['number'],
                        'section_level': 2,
                        'section_type': section_type,
                        'page_start': subsection['page']
                    }
                )
                
                for chunk in subsection_chunks:
                    chunk['chunk_id'] = f"chunk_{chunk_id}"
                    chunk['hierarchy'] = {
                        'parent': f"{section['number']}_{section['title']}",
                        'children': [],
                        'level': 2,
                        'path': f"{section['number']}_{section['title']}/{subsection['number']}_{subsection['title']}"
                    }
                    all_chunks.append(chunk)
                    chunk_id += 1
                    
        return all_chunks
    
    def _create_chunks(self, text: str, config: ChunkConfig, metadata: Dict) -> List[Dict]:
        """
        Create chunks based on configuration
        """
        chunks = []
        
        if not text.strip():
            return chunks
            
        # Tokenize text
        tokens = self.encoder.encode(text)
        
        # Split into sentences for better boundaries
        sentences = self._split_sentences(text)
        
        if config.preserve_sentences:
            chunks = self._chunk_by_sentences(sentences, config, metadata)
        else:
            chunks = self._chunk_by_tokens(tokens, text, config, metadata)
            
        return chunks
    
    def _chunk_by_sentences(self, sentences: List[str], config: ChunkConfig, 
                           metadata: Dict) -> List[Dict]:
        """
        Chunk preserving sentence boundaries
        """
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = len(self.encoder.encode(sentence))
            
            if current_tokens + sentence_tokens > config.max_tokens and current_chunk:
                # Save current chunk
                chunk_text = ' '.join(current_chunk)
                chunks.append({
                    'text': chunk_text,
                    'metadata': metadata.copy(),
                    'token_count': current_tokens
                })
                
                # Start new chunk with overlap
                if config.overlap_tokens > 0:
                    # Keep last few sentences for overlap
                    overlap_text = ' '.join(current_chunk[-2:])
                    overlap_tokens = len(self.encoder.encode(overlap_text))
                    if overlap_tokens < config.overlap_tokens:
                        current_chunk = current_chunk[-2:]
                        current_tokens = overlap_tokens
                    else:
                        current_chunk = []
                        current_tokens = 0
                else:
                    current_chunk = []
                    current_tokens = 0
            
            current_chunk.append(sentence)
            current_tokens += sentence_tokens
        
        # Add remaining chunk
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            chunks.append({
                'text': chunk_text,
                'metadata': metadata.copy(),
                'token_count': current_tokens
            })
            
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences, handling academic text peculiarities
        """
        import re
        
        # Handle common academic abbreviations
        text = re.sub(r'\b(et al|i\.e|e\.g|cf|Fig|Eq|Sec|Ch|Ref|Vol|No|pp|vs)\.\s*', r'\1<PERIOD> ', text)
        
        # Split on sentence boundaries
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
        
        # Restore periods
        sentences = [s.replace('<PERIOD>', '.') for s in sentences]
        
        return [s.strip() for s in sentences if s.strip()]
    
    def _identify_section_type(self, title: str) -> str:
        """
        Identify section type from title
        """
        title_lower = title.lower()
        
        for section_type in self.chunk_configs.keys():
            if section_type in title_lower:
                return section_type
                
        if any(word in title_lower for word in ['model', 'architecture', 'network']):
            return 'model'
        elif any(word in title_lower for word in ['data', 'dataset', 'features']):
            return 'data'
        elif any(word in title_lower for word in ['experiment', 'result', 'performance']):
            return 'results'
            
        return 'default'
4. Metadata Extraction Module
python# src/ingestion/metadata_extractor.py
import re
from typing import Dict, List, Set
import spacy

class FinancialMLMetadataExtractor:
    """
    Extract financial and ML specific metadata from chunks
    """
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        
        # Financial ML entities to extract
        self.ml_models = {
            'lstm', 'gru', 'transformer', 'bert', 'gpt', 'attention',
            'cnn', 'rnn', 'gan', 'vae', 'autoencoder', 'dqn', 'ppo',
            'random forest', 'xgboost', 'lightgbm', 'svm', 'neural network'
        }
        
        self.financial_metrics = {
            'sharpe ratio', 'sortino ratio', 'calmar ratio', 'max drawdown',
            'alpha', 'beta', 'volatility', 'var', 'cvar', 'expected shortfall',
            'information ratio', 'treynor ratio', 'omega ratio', 'profit factor'
        }
        
        self.datasets = {
            'nyse', 'nasdaq', 'sp500', 's&p 500', 'dow jones', 'ftse',
            'crypto', 'bitcoin', 'ethereum', 'forex', 'treasury',
            'high frequency', 'tick data', 'order book', 'lob', 'taq'
        }
        
        self.methodologies = {
            'supervised learning', 'unsupervised learning', 'reinforcement learning',
            'deep learning', 'time series', 'cross validation', 'backtesting',
            'walk forward', 'portfolio optimization', 'risk management',
            'feature engineering', 'automl', 'ensemble', 'transfer learning'
        }
        
    def extract_metadata(self, chunk: Dict) -> Dict:
        """
        Extract all relevant metadata from a chunk
        """
        text = chunk['text'].lower()
        
        # Extract entities
        entities = {
            'ml_models': self._extract_entities(text, self.ml_models),
            'financial_metrics': self._extract_entities(text, self.financial_metrics),
            'datasets': self._extract_entities(text, self.datasets),
            'methodologies': self._extract_entities(text, self.methodologies),
            'tickers': self._extract_tickers(chunk['text']),  # Original case
            'time_periods': self._extract_time_periods(text),
            'numerical_values': self._extract_numerical_values(text)
        }
        
        # Merge with existing metadata
        chunk['metadata'].update({
            'entities': entities,
            'has_code': self._detect_code(chunk['text']),
            'has_equations': self._detect_equations(chunk['text']),
            'has_tables': self._detect_tables(chunk['text']),
            'complexity_score': self._calculate_complexity(entities)
        })
        
        return chunk
    
    def _extract_entities(self, text: str, entity_set: Set[str]) -> List[str]:
        """
        Extract entities from text
        """
        found_entities = []
        for entity in entity_set:
            if entity in text:
                # Count occurrences
                count = text.count(entity)
                found_entities.append({
                    'entity': entity,
                    'count': count
                })
        return found_entities
    
    def _extract_tickers(self, text: str) -> List[str]:
        """
        Extract stock tickers (uppercase 1-5 letter combinations)
        """
        # Common patterns for tickers
        ticker_pattern = r'\b[A-Z]{1,5}\b'
        potential_tickers = re.findall(ticker_pattern, text)
        
        # Filter out common words
        common_words = {'I', 'A', 'THE', 'AND', 'OR', 'FOR', 'TO', 'IN', 'OF', 'II', 'III'}
        tickers = [t for t in potential_tickers if t not in common_words]
        
        return list(set(tickers))
    
    def _extract_time_periods(self, text: str) -> List[str]:
        """
        Extract time periods and dates
        """
        periods = []
        
        # Year ranges
        year_range_pattern = r'(19|20)\d{2}[-–](19|20)\d{2}'
        periods.extend(re.findall(year_range_pattern, text))
        
        # Single years
        year_pattern = r'\b(19|20)\d{2}\b'
        periods.extend(re.findall(year_pattern, text))
        
        # Frequencies
        freq_pattern = r'\b(\d+[-\s]?(?:second|minute|hour|day|week|month|year)s?)\b'
        periods.extend(re.findall(freq_pattern, text))
        
        return list(set(periods))
    
    def _extract_numerical_values(self, text: str) -> Dict:
        """
        Extract numerical values with context
        """
        values = {
            'percentages': re.findall(r'\d+\.?\d*\s*%', text),
            'decimals': re.findall(r'\b\d+\.\d+\b', text),
            'scientific': re.findall(r'\d+\.?\d*[eE][+-]?\d+', text)
        }
        return values
    
    def _detect_code(self, text: str) -> bool:
        """
        Detect if chunk contains code snippets
        """
        code_indicators = [
            r'def\s+\w+\s*\(',  # Python function
            r'class\s+\w+',      # Class definition
            r'import\s+\w+',     # Import statement
            r'for\s+\w+\s+in',   # For loop
            r'if\s+.*:',         # If statement
            r'\w+\s*=\s*\w+\(',  # Function call assignment
        ]
        
        return any(re.search(pattern, text) for pattern in code_indicators)
    
    def _detect_equations(self, text: str) -> bool:
        """
        Detect mathematical equations
        """
        equation_indicators = [
            r'\\[a-zA-Z]+\{',  # LaTeX commands
            r'\$.*\$',          # Inline math
            r'∑', r'∏', r'∫',   # Mathematical symbols
            r'\bsum\b', r'\bprod\b',
            r'[xyzαβγδ]_[it0-9]',  # Subscripted variables
        ]
        
        return any(re.search(pattern, text) for pattern in equation_indicators)
    
    def _detect_tables(self, text: str) -> bool:
        """
        Detect presence of tables
        """
        # Look for table indicators
        table_patterns = [
            r'Table\s+\d+',
            r'\|\s*\w+\s*\|',  # Markdown tables
            r'\t\w+\t',         # Tab-separated values
        ]
        
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in table_patterns)
    
    def _calculate_complexity(self, entities: Dict) -> float:
        """
        Calculate complexity score based on entities
        """
        score = 0.0
        
        # More models mentioned = higher complexity
        score += len(entities.get('ml_models', [])) * 0.2
        
        # Financial metrics indicate quantitative analysis
        score += len(entities.get('financial_metrics', [])) * 0.15
        
        # Multiple methodologies = comprehensive approach
        score += len(entities.get('methodologies', [])) * 0.25
        
        # Normalize to 0-1
        return min(score, 1.0)
Phase 2: Prompt Management System
Prompt Loader Utility
python# src/utils/prompt_loader.py
import os
from pathlib import Path
from typing import Dict, Optional
import yaml

class PromptLoader:
    """
    Load and manage prompts from text files
    """
    
    def __init__(self, prompts_dir: str = "prompts"):
        self.prompts_dir = Path(prompts_dir)
        self.prompts_cache = {}
        self._load_all_prompts()
        
    def _load_all_prompts(self):
        """
        Load all prompts into cache at initialization
        """
        for agent_dir in self.prompts_dir.iterdir():
            if agent_dir.is_dir():
                agent_name = agent_dir.name
                self.prompts_cache[agent_name] = {}
                
                for prompt_file in agent_dir.glob("*.txt"):
                    prompt_name = prompt_file.stem
                    with open(prompt_file, 'r') as f:
                        self.prompts_cache[agent_name][prompt_name] = f.read()
                        
    def get_prompt(self, agent: str, prompt_name: str, **kwargs) -> str:
        """
        Get a prompt and format it with provided variables
        """
        if agent not in self.prompts_cache:
            raise ValueError(f"Agent '{agent}' not found in prompts")
            
        if prompt_name not in self.prompts_cache[agent]:
            raise ValueError(f"Prompt '{prompt_name}' not found for agent '{agent}'")
            
        prompt_template = self.prompts_cache[agent][prompt_name]
        
        # Format the prompt with provided kwargs
        try:
            return prompt_template.format(**kwargs)
        except KeyError as e:
            raise ValueError(f"Missing required variable for prompt: {e}")
            
    def reload_prompts(self):
        """
        Reload all prompts (useful for development)
        """
        self.prompts_cache.clear()
        self._load_all_prompts()
Sample Prompt Files
text# prompts/query_analyzer/classify_query.txt
You are a Financial ML Research Query Analyzer. Classify the following query into one or more categories and extract key information.

Query: {query}

Classification Categories:
1. FACTUAL - Seeking specific information or definitions
2. COMPARATIVE - Comparing models, methods, or results
3. METHODOLOGY - Asking about implementation details
4. APPLICATION - Seeking practical use cases
5. EVALUATION - Asking about performance metrics
6. THEORETICAL - Conceptual or mathematical questions

Output Format (JSON):
{{
    "primary_category": "...",
    "secondary_categories": [...],
    "complexity": "simple|moderate|complex",
    "key_entities": {{
        "models": [...],
        "metrics": [...],
        "concepts": [...]
    }},
    "temporal_scope": "historical|current|future",
    "requires_comparison": true/false,
    "requires_code": true/false
}}

Analysis:
text# prompts/query_analyzer/decompose_query.txt
Break down this complex financial ML query into sub-questions that can be answered independently.

Original Query: {query}

Consider:
1. Are there multiple distinct questions?
2. Does answering require sequential steps?
3. Are different paper sections needed for different parts?

Decomposition Format:
{{
    "sub_queries": [
        {{
            "question": "...",
            "focus": "...",
            "required_sections": ["methodology", "results", ...],
            "priority": "high|medium|low"
        }}
    ],
    "dependencies": [
        {{"from": 0, "to": 1, "relationship": "..."}}
    ],
    "synthesis_required": true/false
}}

Decomposition:
text# prompts/domain_expert/analyze_methodology.txt
As a Financial ML Expert, analyze the methodology described in these paper sections.

Context:
Paper: {paper_title}
Section: {section_name}

Content:
{chunk_text}

Analyze the following aspects:

1. **Model Architecture**
   - What ML models are used?
   - How are they configured?
   - Any novel modifications?

2. **Financial Context**
   - What financial problem is being solved?
   - Time horizon (HRetryKContinueEdittext# prompts/domain_expert/analyze_methodology.txt (continued)
   - Time horizon (HFT, intraday, daily, etc.)?
   - Asset classes covered?

3. **Data & Features**
   - Data sources and frequency
   - Feature engineering approach
   - Missing data handling

4. **Validation Strategy**
   - Cross-validation approach (walk-forward, purged, embargo?)
   - Train/test split methodology
   - Overfitting prevention measures

5. **Risk Considerations**
   - Risk metrics used
   - Transaction costs considered?
   - Market impact modeling?

6. **Strengths & Limitations**
   - What makes this approach robust?
   - Potential weaknesses or assumptions?
   - Generalizability to other markets/periods?

Provide a structured analysis focusing on practical applicability and methodological rigor.

Analysis:
text# prompts/domain_expert/compare_models.txt
Compare the following ML models based on the provided research findings for financial applications.

Models to Compare:
{model_1}: {model_1_description}
{model_2}: {model_2_description}

Evidence from Papers:
{evidence_chunks}

Comparison Framework:

1. **Architecture Differences**
   - Core structural differences
   - Parameter complexity
   - Computational requirements

2. **Financial Performance**
   - Return metrics (Sharpe, Sortino, etc.)
   - Risk metrics (VaR, drawdown)
   - Trading performance (if applicable)

3. **Data Requirements**
   - Minimum data needed
   - Feature types handled well
   - Robustness to noise

4. **Training Characteristics**
   - Convergence speed
   - Stability
   - Hyperparameter sensitivity

5. **Practical Deployment**
   - Inference speed
   - Model interpretability
   - Production considerations

6. **Best Use Cases**
   - When to prefer {model_1}
   - When to prefer {model_2}
   - Hybrid approaches

Synthesize findings with specific citations to paper sections.

Comparison:
text# prompts/retriever/query_expansion.txt
Expand this financial ML query to improve retrieval coverage without losing precision.

Original Query: {query}

Expansion Strategies:
1. Add synonyms for technical terms
2. Include related financial concepts
3. Expand abbreviations
4. Add common variations

Current Entities Identified:
- Models: {models}
- Metrics: {metrics}
- Concepts: {concepts}

Generate expanded queries that would help retrieve relevant chunks:

Format:
{{
    "expanded_queries": [
        {{
            "query": "...",
            "focus": "semantic|keyword|hybrid",
            "weight": 0.0-1.0
        }}
    ],
    "must_include_terms": [...],
    "should_include_terms": [...],
    "exclude_terms": [...]
}}

Expansion:
text# prompts/orchestrator/generate_response.txt
Generate a comprehensive response to the user's financial ML query using the retrieved information.

User Query: {query}

Query Analysis:
{query_analysis}

Retrieved Evidence:
{retrieved_chunks}

Expert Analysis:
{expert_analysis}

Response Guidelines:
1. Start with a direct answer to the main question
2. Provide supporting evidence with inline citations
3. Include practical considerations for financial applications
4. Mention any caveats or limitations
5. Suggest further reading if relevant

Citation Format:
- Use [Paper Title, Section X.Y] for citations
- Ensure every claim is supported by evidence
- Link citations to chunk IDs for verification

Structure:
- **Direct Answer**: 1-2 sentences addressing the core question
- **Detailed Explanation**: Comprehensive analysis with evidence
- **Practical Implications**: Real-world application insights
- **Limitations & Considerations**: Important caveats
- **Related Research**: Other relevant findings if applicable

Response:
text# prompts/orchestrator/validate_response.txt
Validate the generated response for accuracy and completeness.

Original Query: {query}
Generated Response: {response}
Source Chunks: {chunks}

Validation Checklist:

1. **Factual Accuracy**
   - [ ] All claims are supported by source material
   - [ ] No hallucinated information
   - [ ] Numbers and metrics are correctly cited

2. **Citation Quality**
   - [ ] Every major claim has a citation
   - [ ] Citations point to correct chunks
   - [ ] Citation format is consistent

3. **Completeness**
   - [ ] All aspects of the query addressed
   - [ ] No important information omitted
   - [ ] Appropriate level of detail

4. **Financial ML Relevance**
   - [ ] Maintains financial context
   - [ ] Uses appropriate terminology
   - [ ] Considers practical constraints

5. **Clarity**
   - [ ] Response is well-structured
   - [ ] Technical terms explained if needed
   - [ ] Logical flow of ideas

Issues Found:
{{
    "factual_errors": [...],
    "missing_citations": [...],
    "incomplete_answers": [...],
    "suggestions": [...]
}}

Validation Status: PASS/FAIL
Confidence Score: 0.0-1.0
Phase 3: Ingestion Pipeline Script
python# scripts/ingest_papers.py
import argparse
import json
from pathlib import Path
from tqdm import tqdm
import logging
from typing import List, Dict

from src.ingestion.pdf_extractor import PDFExtractor
from src.ingestion.paper_parser import PaperSectionParser
from src.ingestion.chunker import HierarchicalChunker
from src.ingestion.metadata_extractor import FinancialMLMetadataExtractor
from src.indexing.embeddings_generator import EmbeddingsGenerator
from src.indexing.vector_store import ChromaDBStore
from src.indexing.bm25_indexer import BM25Indexer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PaperIngestionPipeline:
    """
    End-to-end pipeline for ingesting financial ML papers
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = self._load_config(config_path)
        
        # Initialize components
        self.extractor = PDFExtractor()
        self.parser = PaperSectionParser()
        self.chunker = HierarchicalChunker()
        self.metadata_extractor = FinancialMLMetadataExtractor()
        self.embeddings_generator = EmbeddingsGenerator()
        self.vector_store = ChromaDBStore(self.config['chroma_db_path'])
        self.bm25_indexer = BM25Indexer(self.config['bm25_index_path'])
        
    def ingest_papers(self, paper_paths: List[Path], 
                      batch_size: int = 100) -> Dict:
        """
        Ingest multiple papers into the system
        """
        stats = {
            'papers_processed': 0,
            'total_chunks': 0,
            'errors': []
        }
        
        for paper_path in tqdm(paper_paths, desc="Ingesting papers"):
            try:
                logger.info(f"Processing {paper_path.name}")
                
                # 1. Extract text from PDF
                extracted_data = self.extractor.extract_from_pdf(str(paper_path))
                
                # 2. Parse paper structure
                structure = self.parser.parse_structure(extracted_data['pages'])
                tagged_sections = self.parser.identify_key_sections(structure)
                
                # 3. Create hierarchical chunks
                chunks = self.chunker.chunk_paper(
                    structure, 
                    extracted_data['metadata']
                )
                
                # 4. Extract metadata for each chunk
                enriched_chunks = []
                for chunk in chunks:
                    enriched_chunk = self.metadata_extractor.extract_metadata(chunk)
                    enriched_chunks.append(enriched_chunk)
                
                # 5. Generate embeddings
                embeddings = self.embeddings_generator.generate_embeddings(
                    [c['text'] for c in enriched_chunks]
                )
                
                # Add embeddings to chunks
                for chunk, embedding in zip(enriched_chunks, embeddings):
                    chunk['embedding'] = embedding
                
                # 6. Store in vector database
                self.vector_store.add_chunks(enriched_chunks)
                
                # 7. Update BM25 index
                self.bm25_indexer.add_documents(
                    [c['text'] for c in enriched_chunks],
                    [c['chunk_id'] for c in enriched_chunks]
                )
                
                # 8. Save processed chunks to JSON for debugging
                output_path = Path(self.config['processed_papers_path']) / f"{paper_path.stem}.json"
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(output_path, 'w') as f:
                    json.dump({
                        'metadata': extracted_data['metadata'],
                        'structure': structure,
                        'chunks': enriched_chunks,
                        'stats': {
                            'num_chunks': len(enriched_chunks),
                            'sections': len(structure['sections'])
                        }
                    }, f, indent=2, default=str)
                
                stats['papers_processed'] += 1
                stats['total_chunks'] += len(enriched_chunks)
                
                logger.info(f"Successfully processed {paper_path.name}: {len(enriched_chunks)} chunks")
                
            except Exception as e:
                logger.error(f"Error processing {paper_path.name}: {str(e)}")
                stats['errors'].append({
                    'paper': paper_path.name,
                    'error': str(e)
                })
        
        # Save ingestion stats
        with open('data/ingestion_stats.json', 'w') as f:
            json.dump(stats, f, indent=2)
            
        return stats
    
    def _load_config(self, config_path: str) -> Dict:
        """
        Load configuration from YAML file
        """
        import yaml
        
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)

def main():
    parser = argparse.ArgumentParser(description='Ingest financial ML papers')
    parser.add_argument(
        '--papers_dir',
        type=str,
        default='data/raw_papers',
        help='Directory containing PDF papers'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config.yaml',
        help='Configuration file path'
    )
    parser.add_argument(
        '--batch_size',
        type=int,
        default=100,
        help='Batch size for processing'
    )
    
    args = parser.parse_args()
    
    # Get all PDF files
    papers_dir = Path(args.papers_dir)
    paper_paths = list(papers_dir.glob('*.pdf'))
    
    if not paper_paths:
        logger.error(f"No PDF files found in {papers_dir}")
        return
    
    logger.info(f"Found {len(paper_paths)} papers to process")
    
    # Run ingestion pipeline
    pipeline = PaperIngestionPipeline(args.config)
    stats = pipeline.ingest_papers(paper_paths, args.batch_size)
    
    # Print summary
    print("\n" + "="*50)
    print("Ingestion Complete!")
    print(f"Papers processed: {stats['papers_processed']}")
    print(f"Total chunks created: {stats['total_chunks']}")
    if stats['errors']:
        print(f"Errors encountered: {len(stats['errors'])}")
        for error in stats['errors']:
            print(f"  - {error['paper']}: {error['error']}")
    print("="*50)

if __name__ == "__main__":
    main()
Phase 4: Configuration File
yaml# config.yaml
# Financial ML Research Assistant Configuration

# Data paths
data_dir: "data"
raw_papers_path: "data/raw_papers"
processed_papers_path: "data/processed_papers"
chroma_db_path: "data/chroma_db"
bm25_index_path: "data/bm25_index"

# Model configurations
embeddings:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  batch_size: 32
  normalize: true

llm:
  model: "gpt-4"
  temperature: 0.1
  max_tokens: 2000

# Chunking parameters
chunking:
  default_max_tokens: 400
  default_overlap: 50
  preserve_sentences: true
  include_tables: true
  include_equations: true

# Retrieval parameters
retrieval:
  semantic_weight: 0.7  # 70% semantic, 30% keyword
  initial_k: 20         # Retrieve top 20 chunks initially
  rerank_k: 10         # Keep top 10 after reranking
  expand_context: true  # Include parent/child chunks
  max_context_tokens: 4000

# Agent parameters
agents:
  query_analyzer:
    model: "gpt-4"
    temperature: 0.1
  
  domain_expert:
    model: "gpt-4"
    temperature: 0.2
    
  orchestrator:
    model: "gpt-4"
    temperature: 0.1
    max_iterations: 3

# Evaluation settings
evaluation:
  test_queries_path: "data/test_queries.json"
  metrics:
    - citation_accuracy
    - answer_completeness
    - hallucination_rate
  output_path: "evaluation_results"

# Logging
logging:
  level: "INFO"
  file: "logs/ingestion.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
Usage Instructions

Place your PDF papers in data/raw_papers/
Run the ingestion pipeline:

bashpython scripts/ingest_papers.py --papers_dir data/raw_papers

Monitor the ingestion:


Check ingestion_stats.json for summary
Review data/processed_papers/*.json for processed chunks
Verify embeddings in ChromaDB


Test the system:

pythonfrom src.agents.orchestrator import FinanceMLOrchestrator
from src.utils.prompt_loader import PromptLoader

# Initialize
orchestrator = FinanceMLOrchestrator()
prompt_loader = PromptLoader()

# Process a query
query = "How do LSTM models handle volatility prediction in high-frequency trading?"
response = await orchestrator.process_query(query)
print(response)