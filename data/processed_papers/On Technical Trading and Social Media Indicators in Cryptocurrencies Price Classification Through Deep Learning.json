{
  "metadata": {
    "title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
    "abstract": "Predicting prices of cryptocurrencies is a notoriously hard task due to the presence of high volatility\nand new mechanisms characterising the crypto markets. In this work we focus on the two major\ncryptocurrencies for market capitalization at the time of the study, Ethereum and Bitcoin, for the\nperiod 2017-2020. We present a comprehensive analysis of the predictability of price movements\ncomparing four different deep learning algorithms (Multi Layers Perceptron (MLP), Convolutional\nNeural Network (CNN), Long Short Term Memory (LSTM) neural network and Attention Long Short\nTerm Memory (ALSTM)) and using three classes of features. In particular, we consider a combination\nof technical (e.g. open and close prices), trading (e.g. moving averages) and social (e.g. users\u2019\nsentiment) indicators used as input to our classi\ufb01cation algorithm. We compare a restricted model\ncomposed of technical indicators only, and an unrestricted model including technical, trading and\nsocial media indicators. The results show that the unrestricted model outperforms the restricted one,\ni.e. including trading and social media indicators, along with the classic technical variables, leads to\na signi\ufb01cant improvement in the prediction accuracy consistently across all algorithms.\nKeywords : Cryptocurrency, Deep Learning, Social Media Indicators, Trading Indicators, Arti\ufb01cial Neural Networks\n1",
    "year": 2021
  },
  "chunks": [
    {
      "text": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021\nON TECHNICAL TRADING AND SOCIAL MEDIA INDICATORS IN\nCRYPTOCURRENCY PRICE CLASSIFICATION THROUGH DEEP\nLEARNING\nA PREPRINT\nMarco Ortu\nUniversity of Cagliari\nmarco.ortu@unica.it\nNicola Uras\nUniversity of Cagliari\nnicola.uras@unica.it\nClaudio Conversano\nUniversity of Cagliari\nconversano@unica.it\nGiuseppe Destefanis\nBrunel University\ngiuseppe.destefanis@brunel.ac.uk\nSilvia Bartolucci\nUniversity College London\ns.bartolucci@ucl.ac.uk\nFebruary 18, 2021\nABSTRACT\nPredicting prices of cryptocurrencies is a notoriously hard task due to the presence of high volatility\nand new mechanisms characterising the crypto markets. In this work we focus on the two major\ncryptocurrencies for market capitalization at the time of the study, Ethereum and Bitcoin, for the\nperiod 2017-2020. We present a comprehensive analysis of the predictability of price movements\ncomparing four different deep learning algorithms (Multi Layers Perceptron (MLP), Convolutional\nNeural Network (CNN), Long Short Term Memory (LSTM) neural network and Attention Long Short\nTerm Memory (ALSTM)) and using three classes of features. In particular, we consider a combination\nof technical (e.g. open and close prices), trading (e.g. moving averages) and social (e.g. users\u2019\nsentiment) indicators used as input to our classi\ufb01cation algorithm. We compare a restricted model\ncomposed of technical indicators only, and an unrestricted model including technical, trading and\nsocial media indicators.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "1 Introduction",
        "level": 1,
        "page_start": 1
      },
      "token_count": 383,
      "chunk_id": "chunk_0"
    },
    {
      "text": "We compare a restricted model\ncomposed of technical indicators only, and an unrestricted model including technical, trading and\nsocial media indicators. The results show that the unrestricted model outperforms the restricted one,\ni.e. including trading and social media indicators, along with the classic technical variables, leads to\na signi\ufb01cant improvement in the prediction accuracy consistently across all algorithms. Keywords : Cryptocurrency, Deep Learning, Social Media Indicators, Trading Indicators, Arti\ufb01cial Neural Networks\n1\nIntroduction\nDuring the last decade, the global markets have witnessed the rise and exponential growth of cryptocurrencies traded\nand exchanged with a daily market capitalization of hundreds of billions of USD Dollars globally (reaching \u22481 trillion\nas of January 2021). Recent surveys1 report a spike in demand and interest for the new crypto assets from institutional investors, attracted\nby the novel features and the potential rise in value in the current \ufb01nancial turmoil, despite the risk associated with\nprice volatility and market manipulation. Boom and bust cycles often induced by network effects and wider market\u2019s adoption, make prices hard to predict\nwith high accuracy. There is a large body of literature concerning this issue and proposing a number of quantitative\napproaches for cryptocurrency prices prediction [13,15\u201318]. The rapid \ufb02uctuations in volatility, autocorrelations and\nmulti-scaling effects in cryptocurrencies have also been extensively studied [22], also with respect to their effect on\nInitial Coin Offering (ICO) [10,11].\n1See Fidelity Report.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "1 Introduction",
        "level": 1,
        "page_start": 1
      },
      "token_count": 316,
      "chunk_id": "chunk_1"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nAn important consideration that has gradually emerged from the literature is the relevance of the \u201csocial aspect\u201d of\ncrypto trading. The code underlying blockchain platforms is developed in an open-source fashion on Github, recent\nadditions to the crypto ecosystem are discussed on Reddit or on specialised channels in Telegram, and Twitter offers\na platform where often heated debates on the latest developments take place. More precisely, it has been shown that\nsentiment index can be used to predict bubbles in prices [5] and that the sentiment extracted from topic discussions on\nReddit correlates with prices [28]. Open-source development also plays an important role in shaping the success and value of cryptocurrencies [21,25,27]. In particular, a previous work by Bartolucci et al. [2] \u2013 which this work is an extension of \u2013 showed the existence of\na Granger causality between the sentiment and emotions time series extracted from developers\u2019 comments on Github\nand returns of cryptocurrencies. For the two major cryptocurrencies \u2013 Bitcoin and Ethereum \u2013 it has been also shown\nhow including the developers\u2019 emotions time series in prediction algorithms could substantially improve the accuracy. In this paper, we further extend previous investigations on price predictability using a deep learning approach and\nfocusing on the two major cryptocurrencies by market capitalization, Bitcoin and Ethereum. We predict price movements by mapping the punctual price forecasting into a classi\ufb01cation problem: our target is a\nbinary variable with two unique classes, upward and downward movements, which indicate prices rising or falling. In the following we will compare the performances and outcome of four deep learning algorithms: the Multi-Layer\nPerceptron (MLP), the Multivariate Attention Long Short Term Memory Fully Convolutional Network (MALSTM-\nFCN), the Convolutional Neural Network (CNN) and the Long Short Term Memory neural network (LSTM).",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.1 Technical Indicators",
        "level": 2,
        "page_start": 2
      },
      "token_count": 390,
      "chunk_id": "chunk_2"
    },
    {
      "text": "In the following we will compare the performances and outcome of four deep learning algorithms: the Multi-Layer\nPerceptron (MLP), the Multivariate Attention Long Short Term Memory Fully Convolutional Network (MALSTM-\nFCN), the Convolutional Neural Network (CNN) and the Long Short Term Memory neural network (LSTM). We will use as input the following classes of (\ufb01nancial and social) indicators: (i) technical indicators, such as open\nand close price or volume traded, (ii) trading indicators, such as the momentum and moving averages calculated on\nthe price, (iii) social media indicators, i.e. sentiment and emotions extracted from Github and Reddit comments. For each deep learning algorithm we consider a restricted and unrestricted data model at a hourly and daily frequency. The restricted model consists of data concerning technical variables for Bitcoin and Ethereum. In the unrestricted\nmodel we include, instead, the technical variables, trading and social media indicators from Github and Reddit. Consistently across all four deep learning algorithms, we are able to show that that the unrestricted model outperforms\nthe restricted model. At hourly data frequency, the inclusion of trading and social media indicators alongside the\nclassic technical indicators improves the accuracy on Bitcoin and Ethereum price prediction, increasing from a range\nof 51-55% for the restricted model to 67-84% for the unrestricted one. For the daily frequency resolution, in the case\nof Ethereum the most accurate classi\ufb01cation is achieved using the restricted model. For Bitcoin, instead, the highest\nperformance is achieved for the unrestricted model including only social media indicators. In the following sections we will discuss in details the algorithms implemented and the Bootstrap validation technique\nused to estimate the performance of the models. The paper is organised as follows. In Section 2 we describe in detail the data and indicators used. In Section 3, we\ndiscuss the methodology of the experiments conducted.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.1 Technical Indicators",
        "level": 2,
        "page_start": 2
      },
      "token_count": 397,
      "chunk_id": "chunk_3"
    },
    {
      "text": "In Section 3, we\ndiscuss the methodology of the experiments conducted. In Section 4 we present the results and their implications and\nin Section 5 we discuss the limitations of this study. Finally, in Section 6 we summarise our \ufb01ndings and outline future\ndirections.\n2\nDataset: Technical and Social Media Indicators\nThis section discusses the dataset and the three categories of indicators used for the experiments.\n2.1\nTechnical Indicators\nWe conducted our analysis on Bitcoin and Ethereum price time series with an hourly and daily frequency resolution. We considered all the available technical variables, extracted from the Crypto Data Download web services2, in partic-\nular the data from Bit\ufb01nex.com exchange3 service. We considered the last 4-year period, spanning from 2017/01/01\nto 2021/01/01, for a total of 35638 hourly observations. In our analysis, we separate the technical indicators into two main categories: pure technical and trading indicators. Technical indicators refer to \u201cdirect\u201d market data such as opening and closing prices. Trading indicators refer to\nderived indicators such as the moving averages. The technical indicators are listed below.\n2https://www.cryptodatadownload.com/data/bit\ufb01nex/\n3https://www.bit\ufb01nex.com/\n2\nA PREPRINT - FEBRUARY 18, 2021\n\u2022 Close: the last price at which the cryptocurrency traded during the trading period.\n\u2022 Open: the price at which the cryptocurrency \ufb01rst trades upon the opening of a trading period.\n\u2022 Low: the lowest price at which the cryptocurrency trades over the course of a trading period.\n\u2022 High: the highest price at which the cryptocurrency traded during the course of the trading period.\n\u2022 Volume: the number of cryptocurrency trades completed. Tables 1 and 2 show the summary statistics for the technical indicators.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.1 Technical Indicators",
        "level": 2,
        "page_start": 2
      },
      "token_count": 388,
      "chunk_id": "chunk_4"
    },
    {
      "text": "Tables 1 and 2 show the summary statistics for the technical indicators. In Figures 1 and 2 we also show the plot of\nthe historical time series for the technical indicators. High\nOpen\nLow\nVolume\nClose\nmean\n7972.769\n7928.018\n7879.276\n176.319\n7928.894\nstd\n5519.337\n5471.592\n5416.295\n306.62\n5472.983\nmin\n769.1\n760.38\n752\n0\n760.38\n25%\n4161.6875\n4137.995\n4113.822\n30.592\n4138.475\n50%\n7459.995\n7428.09\n7390.47\n80.699\n7428.41\n75%\n9790.952\n9751.84\n9701.427\n199.322\n9752.37\nmax\n41999.99\n41526.95\n41000.24\n8526.751\n41526.95\nTable 1: Summary statistics for the time series of Bitcoin\u2019s technical indicators. Figure 1: Plot of the time series of Bitcoin\u2019s technical indicators. From the knowledge of these technical indicators it is possible to calculate the trading indicators. More precisely, we\nused the StockStats Python library to generate them. We used 36 different trading indicators as shown in Table 4. The lag values represent how previous values (t \u2212\n1, . . . , t \u2212n) are used as input. The window size indicates the number of previous values used to evaluate the indicator\nat time t, e.g. to calculate ADXRt at time t we use ADXt\u22121, ..., ADXRt\u221210, ten previous values.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.1 Technical Indicators",
        "level": 2,
        "page_start": 2
      },
      "token_count": 375,
      "chunk_id": "chunk_5"
    },
    {
      "text": "The window size indicates the number of previous values used to evaluate the indicator\nat time t, e.g. to calculate ADXRt at time t we use ADXt\u22121, ..., ADXRt\u221210, ten previous values. We provide here the de\ufb01nition of the \ufb01ve main trading indicators.\n\u2022 Simple Moving Average (SMA): calculated as the arithmetic average of the cryptocurrency closing price\nover some period (known as timeperiod).\n3",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.1 Technical Indicators",
        "level": 2,
        "page_start": 2
      },
      "token_count": 96,
      "chunk_id": "chunk_6"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nHigh\nOpen\nLow\nVolume\nClose\nmean\n313.202\n310.856\n308.253\n1658.835\n310.896\nstd\n248.731\n246.069\n242.971\n6903.628\n246.135\nmin\n8.17\n8.15\n8.15\n0\n8.15\n25%\n161.182\n160.202\n159.06\n192.327\n160.21\n50%\n232.79\n231.34\n229.765\n569.79\n231.365\n75%\n390.0575\n388.0075\n385.73\n1632.640\n388.025\nmax\n1440.54\n1430.94\n1411\n903102.685\n1431.4\nTable 2: Summary statistics for the time series of Ethereum\u2019s technical indicators. Figure 2: Plot of the time series of Ethereum\u2019s technical indicators.\n\u2022 Weighted Moving Average (WMA): it is a moving average calculation that assigns higher weights to the\nmost recent price data.\n\u2022 Relative Strength Index (RSI): it is a momentum indicator that measures the magnitude of recent price\nchanges. It is normally used to evaluate whether stocks or other assets are being overbought or oversold.\n\u2022 Price Rate Of Change (ROC): it measures the percentage change in price between the current price and the\nprice a certain number of periods ago.\n\u2022 Momentum: it is the rate of acceleration of a security\u2019s price, i.e. the speed at which the price is changing. This measure is particularly useful to identify trends.\n\u2022 On Balance Volume (OBV ): it is a technical momentum indicator based on the traded volume of an asset to\npredict changes in stock price.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.2 Social Media Indicators",
        "level": 2,
        "page_start": 4
      },
      "token_count": 382,
      "chunk_id": "chunk_7"
    },
    {
      "text": "This measure is particularly useful to identify trends.\n\u2022 On Balance Volume (OBV ): it is a technical momentum indicator based on the traded volume of an asset to\npredict changes in stock price. Tables 3 and 5 show the statistics of the trading indicators for the considered period of analysis. In Figures 3 and 4\nwe can see the same trading indicators in a historical time series plot. Technical and Trading indicators are used in the\nnext sections to create a model for the prices classi\ufb01cation.\n2.2\nSocial Media Indicators\nThis section describes how the time series of social media indicators are constructed from Ethereum and Bitcoin\ndevelopers comments on Github and users\u2019 comments on Reddit respectively. In particular, for Reddit we considered\nthe four sub-Reddit channels listed in Table 6.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.2 Social Media Indicators",
        "level": 2,
        "page_start": 4
      },
      "token_count": 164,
      "chunk_id": "chunk_8"
    },
    {
      "text": "In particular, for Reddit we considered\nthe four sub-Reddit channels listed in Table 6. The time period considered ranges from January 2017 to January 2021.\n4\nA PREPRINT - FEBRUARY 18, 2021\nSMA\nWMA\nRSI\nROCP\nMOM\nOBV\nmean\n7924.974\n7926.275\n51.797\n0.0013\n8.685\n126972.751\nstd\n5465.393\n5467.642\n14.484\n0.027\n298.311\n33544.231\nmin\n767.801\n766.912\n2.426\n-0.321\n-5260.55\n18811.069954\n25%\n4135.225\n4134.525\n42.374\n-0.0083\n-53.64\n110336.0947\n50%\n7427.187\n7427.521\n51.877\n0.001\n4.97\n126464.383\n75%\n9753.094\n9751.604\n61.151\n0.011\n70.732\n147814.358\nmax\n40996.6\n41106.93\n98.641\n0.314\n4069.26\n213166.214\nTable 3: Summary statistics for the time series of Bitcoin\u2019s trading indicators.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.2 Social Media Indicators",
        "level": 2,
        "page_start": 4
      },
      "token_count": 298,
      "chunk_id": "chunk_9"
    },
    {
      "text": "The time period considered ranges from January 2017 to January 2021.\n4\nA PREPRINT - FEBRUARY 18, 2021\nSMA\nWMA\nRSI\nROCP\nMOM\nOBV\nmean\n7924.974\n7926.275\n51.797\n0.0013\n8.685\n126972.751\nstd\n5465.393\n5467.642\n14.484\n0.027\n298.311\n33544.231\nmin\n767.801\n766.912\n2.426\n-0.321\n-5260.55\n18811.069954\n25%\n4135.225\n4134.525\n42.374\n-0.0083\n-53.64\n110336.0947\n50%\n7427.187\n7427.521\n51.877\n0.001\n4.97\n126464.383\n75%\n9753.094\n9751.604\n61.151\n0.011\n70.732\n147814.358\nmax\n40996.6\n41106.93\n98.641\n0.314\n4069.26\n213166.214\nTable 3: Summary statistics for the time series of Bitcoin\u2019s trading indicators. Trading Indicator\nLag\nWindow size\nSMA: Simple Moving Average\n-\n10\nWMA: Weighted Moving Average\n-\n10\nRSI: Relative Strength Index\n-\n10\nROC: Price Rate Of Change\n-\n10\nMo: Momentum:\n-\n10\nOBV: On Balance Volume\n1\n-\npermutation (zero based)\n1\n-\nlog return\n1\n-\nmax in range\n1\n-\nmin in range\n1\n-\nmiddle = (close + high + low) / 3\n1\n-\ncompare: le, ge, lt, gt, eq, ne\n1\n-\ncount: both backward(c) and forward(fc)\n1\n-\nSMA: simple moving average\n-\n10\nEMA: exponential moving average\n-\n10\nMSTD: moving standard deviation\n-\n10\nMVAR: moving variance\n-\n10\nRSV: raw stochastic value\n-\n10\nRSI: relative strength index\n-\n10\nKDJ: Stochastic oscillator\n-\n10\nBolling: including upper band and lower band.\n1\n-\nMACD: moving average convergence divergence\n-\n5\nCR: price momentum index\n1\n-\nWR: Williams Overbought/Oversold index\n1\n-\nCCI: Commodity Channel Index\n1\n-\nTR: true range\n1\n-\nATR: average true range\n1\n-\nline cross check, cross up or cross down.\n1\n-\nDMA: Different of Moving Average (10, 50)\n1\n-\nDMI: Directional Moving Index, including\n1\n-\nDI: Positive Directional Indicator\n1\n-\nADX: Average Directional Movement Index\n-\n5\nADXR: Smoothed Moving Average of ADX\n-\n10\nTRIX: Triple Exponential Moving Average\n-\n10\nTEMA: Another Triple Exponential Moving Average\n-\n10\nVR: Volatility Volume Ratio\n1\n-\nTable 4: Trading indicators with associated lags and window size.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.2 Social Media Indicators",
        "level": 2,
        "page_start": 4
      },
      "token_count": 688,
      "chunk_id": "chunk_10"
    },
    {
      "text": "Trading Indicator\nLag\nWindow size\nSMA: Simple Moving Average\n-\n10\nWMA: Weighted Moving Average\n-\n10\nRSI: Relative Strength Index\n-\n10\nROC: Price Rate Of Change\n-\n10\nMo: Momentum:\n-\n10\nOBV: On Balance Volume\n1\n-\npermutation (zero based)\n1\n-\nlog return\n1\n-\nmax in range\n1\n-\nmin in range\n1\n-\nmiddle = (close + high + low) / 3\n1\n-\ncompare: le, ge, lt, gt, eq, ne\n1\n-\ncount: both backward(c) and forward(fc)\n1\n-\nSMA: simple moving average\n-\n10\nEMA: exponential moving average\n-\n10\nMSTD: moving standard deviation\n-\n10\nMVAR: moving variance\n-\n10\nRSV: raw stochastic value\n-\n10\nRSI: relative strength index\n-\n10\nKDJ: Stochastic oscillator\n-\n10\nBolling: including upper band and lower band.\n1\n-\nMACD: moving average convergence divergence\n-\n5\nCR: price momentum index\n1\n-\nWR: Williams Overbought/Oversold index\n1\n-\nCCI: Commodity Channel Index\n1\n-\nTR: true range\n1\n-\nATR: average true range\n1\n-\nline cross check, cross up or cross down.\n1\n-\nDMA: Different of Moving Average (10, 50)\n1\n-\nDMI: Directional Moving Index, including\n1\n-\nDI: Positive Directional Indicator\n1\n-\nADX: Average Directional Movement Index\n-\n5\nADXR: Smoothed Moving Average of ADX\n-\n10\nTRIX: Triple Exponential Moving Average\n-\n10\nTEMA: Another Triple Exponential Moving Average\n-\n10\nVR: Volatility Volume Ratio\n1\n-\nTable 4: Trading indicators with associated lags and window size. Lags represent how previous values at (t\u22121, . . ., t\u2212\nn) are used as input.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.2 Social Media Indicators",
        "level": 2,
        "page_start": 4
      },
      "token_count": 436,
      "chunk_id": "chunk_11"
    },
    {
      "text": "Lags represent how previous values at (t\u22121, . . ., t\u2212\nn) are used as input. Window size represents the number of previous values used to compute the indicator at time t,\ne.g. to calculate ADXRt at time t we use ADXt\u22121, . . . , ADXRt\u221210.\n5\nA PREPRINT - FEBRUARY 18, 2021\nFigure 3: Plot of the time series of Bitcoin trading indicators. SMA\nWMA\nRSI\nROCP\nMOM\nOBV\nmean\n310.723\n310.78\n51.18\n0.002\n0.378\n6.776e+05\nstd\n245.768\n245.87\n14.246\n0.035\n16.607\n5.739e+05\nmin\n8.147\n8.163\n3.797\n-0.317\n-239.48\n-4.993e+04\n25%\n160.202\n160.252\n42.063\n-0.012\n-2.8\n9.864e+04\n50%\n230.916\n230.962\n51.038\n0.000934\n0.09\n5.185e+05\n75%\n388.081\n388.125\n60.338\n0.016\n3.67\n1.246e+06\nmax\n1404.89\n1411.776\n95.799\n0.333\n262.36\n1.667e+06\nTable 5: Summary statistics for the time series of Ethereum\u2019s trading indicators. Cryptocurrency\nTechnical Discussions\nTrading Discussions\nBitcoin\nr/Bitcoin\nr/BitcoinMarkets\nEthereum\nr/Ethereum\nr/EthTrader\nTable 6: List of sub-Reddit channels considered in the analysis.\n6",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.2 Social Media Indicators",
        "level": 2,
        "page_start": 4
      },
      "token_count": 399,
      "chunk_id": "chunk_12"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nFigure 4: Plot of the time Series of Ethereum trading indicators. Examples of a developer\u2019s comment extracted from Github for Ethereum and user\u2019s comment extracted from Reddit\nr/Ethereum can be seen in Tables 7, 8. Quantitative measures of sentiment and emotions associated with the comments,\nas reported in this example, are computed using state-of-the-art textual analysis tools (further detailed below). These\nsocial media indicators computed for each comment are emotions as love (L), joy (J), anger (A), sadness (S), VAD\n(valence (Val), dominance (Dom), arousal (Ar)) and sentiment (Sent). Comment\nL\nJ\nA\nS\nVal\nDom\nAr\nSent\nPerhaps there\u2019s simply nothing\nnew to translate? The reason\nI updated Transifex in the \ufb01rst\nplace was to be sure the strings\nwith subtle English changes\n(that don\u2019t change the meaning)\ndidn\u2019t reset the translation - so\nthose were imported from the\nold translations. Though I seem\nto recall at least one truly new\nstring - Transaction or such.\n0\n0\n0\n1\n1.93\n1.88\n1.26\n0\nTable 7: Example of a Github comment and corresponding emotions (love (L), joy (J), anger (A), sadness (S)), VAD\n(valence (Val), dominance (Dom), arousal (Ar)), politeness and sentiment (Pol and Sent respectively).\n2.3\nSocial Media Indicators Evaluation Through Deep Learning\nWe extracted the social media indicators using deep, pre-trained, neural networks called Bidirectional Encoder Repre-\nsentations from Transformers (BERT) [8].",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 364,
      "chunk_id": "chunk_13"
    },
    {
      "text": "Though I seem\nto recall at least one truly new\nstring - Transaction or such.\n0\n0\n0\n1\n1.93\n1.88\n1.26\n0\nTable 7: Example of a Github comment and corresponding emotions (love (L), joy (J), anger (A), sadness (S)), VAD\n(valence (Val), dominance (Dom), arousal (Ar)), politeness and sentiment (Pol and Sent respectively).\n2.3\nSocial Media Indicators Evaluation Through Deep Learning\nWe extracted the social media indicators using deep, pre-trained, neural networks called Bidirectional Encoder Repre-\nsentations from Transformers (BERT) [8]. BERT and other Transformer encoder architectures have been successful in\nperforming various tasks in natural language processing (NLP) and represent the evolution of Recurrent Neural Net-\nwork (RNN) typically used in NLP. They compute vector-space representations of natural language that are suitable\nfor use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process\neach token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Rep-\nresentations from Transformers. BERT models are usually pre-trained on a large corpus of text, then \ufb01ne-tuned for\n7\nA PREPRINT - FEBRUARY 18, 2021\nComment\nL\nJ\nA\nS\nVal\nDom\nAr\nSent\nAll the tosspots focusing on Vi-\ntaliks wealth completely miss\nthe point. If the crypto you are\nsupporting has a purpose it will\ngarner interest in the real world\ntherefore the capital will \ufb02ow\nto it.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 355,
      "chunk_id": "chunk_14"
    },
    {
      "text": "If the crypto you are\nsupporting has a purpose it will\ngarner interest in the real world\ntherefore the capital will \ufb02ow\nto it. All is measured on the\nmerit and proper fundamentals\nand not twitterbot pump and\ndumps...\n0\n0\n0\n0\n2.13\n1.98\n2.26\n-1\nTable 8: Example of Reddit comment and correspondent emotions (love (L), joy (J), anger (A), sadness (S)), VAD\n(valence (Val), dominance (Dom), arousal (Ar)), politeness and sentiment (Pol and Sent respectively).\nspeci\ufb01c tasks. These models provide dense vector representations for natural language by using a deep, pre-trained\nneural network with the Transformer architecture represented in Figure 5. Transformers are based on the Attention Mechanism where RNN units would encode the input up until timestamp\nt into one hidden vector ht. The latter would then be passed to the next timestamp (or to the decoder in the case\nof a sequence-to-sequence model). By using the attention mechanism, one no longer tries to encode the full source\nsentence into a \ufb01xed-length vector. Instead, one allows the decoder to attend to different parts of the source sentence\nat each step of the output generation. Importantly, we let the model learn what to attend to based on the input sentence\nand what it has produced so far. The Transformer architecture allows for the creation of NLP models trained on very large datasets as we have done\nin this work. It is feasible to train such models on large datasets thanks to pre-trained language models, which can be\n\ufb01ne-tuned on the particular dataset without the effort of re-training the whole network.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 366,
      "chunk_id": "chunk_15"
    },
    {
      "text": "It is feasible to train such models on large datasets thanks to pre-trained language models, which can be\n\ufb01ne-tuned on the particular dataset without the effort of re-training the whole network. The weights learnt by the extensively pre-trained models can be later reused for speci\ufb01c tasks by simply tailoring the\nweights to the speci\ufb01c dataset. This would allow us to exploit what the pre-trained language model has learnt with a\n\ufb01ner weight tuning by capturing the lower-level intricacies of the speci\ufb01c dataset. We used Tensor\ufb02ow and Keras Python libraries with the Transformer package to leverage the power of these pre-\ntrained neural networks. In particular, we used the BERT-base-case pre-trained model. Figure 6 shows the architectural\ndesign used to train the three NN classi\ufb01ers used to extract the social media indicators. This \ufb01gure shows the three\ngold datasets used to train our \ufb01nal models, namely Github, Stack Over\ufb02ow and Reddit. In particular, we used a sentiment-labelled dataset consisting of 4423 posts mined from Stackover\ufb02ow user\u2019s comments\nto train the sentiment model for Github: comments on both platforms are written using the technical jargon language\nof software developers and engineers. We also used an emotion-labelled dataset of 4200 sentences from Github [23]. Finally, we used a sentiment-labelled dataset containing more than 33K labelled Reddit users\u2019s comments4.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 307,
      "chunk_id": "chunk_16"
    },
    {
      "text": "Finally, we used a sentiment-labelled dataset containing more than 33K labelled Reddit users\u2019s comments4. Tables 9, 10 and 11 show the performance of sentiment and emotion classi\ufb01cation on the two different dataset: Github\nand Reddit.\nprecision\nrecall\nf1-score\nnegative\n0.92\n0.89\n0.90\nneutral\n0.97\n0.98\n0.98\npositive\n0.95\n0.95\n0.95\naccuracy\n0.95\nmacro avg\n0.95\n0.94\n0.94\nweighted avg\n0.95\n0.95\n0.95\nTable 9: Sentiment Classi\ufb01er Evaluation For Reddit.\n4https://www.kaggle.com/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n8\nA PREPRINT - FEBRUARY 18, 2021\nFigure 5: Transformer architectural scheme of Vaswani et al. [29].\nprecision\nrecall\nf1-score\nnegative\n0.98\n0.85\n0.91\nneutral\n0.84\n0.94\n0.89\npositive\n0.96\n0.97\n0.96\naccuracy\n0.92\nmacro avg\n0.93\n0.92\n0.92\nweighted avg\n0.93\n0.92\n0.92\nTable 10: Sentiment Classi\ufb01er Evaluation For Github.\n9\nA PREPRINT - FEBRUARY 18, 2021\nFigure 6: Scheme of the general bidirectional encoder representation from Transformer.\nprecision\nrecall\nf1-score\nanger\n0.83\n0.77\n0.80\nsadness\n0.89\n0.89\n0.89\njoy\n0.86\n1.00\n0.92\nlove\n1.00\n1.00\n1.00\naccuracy\n0.89\nmacro avg\n0.89\n0.91\n0.90\nweighted avg\n0.89\n0.89\n0.89\nTable 11: Emotion Classi\ufb01er Evaluation For Github.\n10\nA PREPRINT - FEBRUARY 18, 2021\n2.3.1\nSocial Media Indicators on Github\nBoth the Bitcoin and Ethereum projects are open-source, hence the code and all the interactions among contributors\nare publicly available on GitHub [26].",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 519,
      "chunk_id": "chunk_17"
    },
    {
      "text": "Tables 9, 10 and 11 show the performance of sentiment and emotion classi\ufb01cation on the two different dataset: Github\nand Reddit.\nprecision\nrecall\nf1-score\nnegative\n0.92\n0.89\n0.90\nneutral\n0.97\n0.98\n0.98\npositive\n0.95\n0.95\n0.95\naccuracy\n0.95\nmacro avg\n0.95\n0.94\n0.94\nweighted avg\n0.95\n0.95\n0.95\nTable 9: Sentiment Classi\ufb01er Evaluation For Reddit.\n4https://www.kaggle.com/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n8\nA PREPRINT - FEBRUARY 18, 2021\nFigure 5: Transformer architectural scheme of Vaswani et al. [29].\nprecision\nrecall\nf1-score\nnegative\n0.98\n0.85\n0.91\nneutral\n0.84\n0.94\n0.89\npositive\n0.96\n0.97\n0.96\naccuracy\n0.92\nmacro avg\n0.93\n0.92\n0.92\nweighted avg\n0.93\n0.92\n0.92\nTable 10: Sentiment Classi\ufb01er Evaluation For Github.\n9\nA PREPRINT - FEBRUARY 18, 2021\nFigure 6: Scheme of the general bidirectional encoder representation from Transformer.\nprecision\nrecall\nf1-score\nanger\n0.83\n0.77\n0.80\nsadness\n0.89\n0.89\n0.89\njoy\n0.86\n1.00\n0.92\nlove\n1.00\n1.00\n1.00\naccuracy\n0.89\nmacro avg\n0.89\n0.91\n0.90\nweighted avg\n0.89\n0.89\n0.89\nTable 11: Emotion Classi\ufb01er Evaluation For Github.\n10\nA PREPRINT - FEBRUARY 18, 2021\n2.3.1\nSocial Media Indicators on Github\nBoth the Bitcoin and Ethereum projects are open-source, hence the code and all the interactions among contributors\nare publicly available on GitHub [26]. Active contributors are continuously opening, commenting, and closing the so-\ncalled \u201cissues\u201d.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 515,
      "chunk_id": "chunk_18"
    },
    {
      "text": "Active contributors are continuously opening, commenting, and closing the so-\ncalled \u201cissues\u201d. An issue is an element of the development process, which carries information about discovered bugs,\nsuggestions on new functionalities to be implemented in the code, new features, or new functionalities being developed. It constitutes an elegant and ef\ufb01cient way of tracking all the development process phases, even in complicated and\nlarge-scale projects with a large number of remote developers involved. An issue can be \u201ccommented\u201d, meaning that\ndevelopers can start sub-discussions around it. They usually add comments to a given issue to highlight the actions\nbeing undertaken or provide suggestions on the possible resolution. Each comment posted on GitHub is timestamped;\ntherefore it is possible to obtain the exact time and date and generate a time series for each affect metric considered in\nthis study. For emotion detection we use the BERT classi\ufb01er explained in 2.3 trained with the public Github\u2019s emotion dataset\ndeveloped by Ortu et al. [24] and extended by Murgia et al. [23]. This dataset is particularly suited for our analysis as\nthe algorithm for emotion detection has been trained on developers\u2019 comments extracted from the Jira Issue Tracking\nSystem5 of the Apache Software Foundation, hence within the Software Engineering domain and context of Github\nand Reddit (considering the selected subreddits). The classi\ufb01er can detect love, anger, joy and sadness with an F1\nscore6 close to 0.89 for all of them. Valence, Arousal and Dominance (VAD) represent conceptualised affective dimensions that respectively describe\nthe interest, alertness and control a subject feels in response to a particular stimulus. In the context of software\ndevelopment, VAD measures may indicate the involvement of a developer in a project as well as their con\ufb01dence and\nresponsiveness in completing tasks.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 391,
      "chunk_id": "chunk_19"
    },
    {
      "text": "In the context of software\ndevelopment, VAD measures may indicate the involvement of a developer in a project as well as their con\ufb01dence and\nresponsiveness in completing tasks. Warriner et al.\u2019s [30] has created a reference lexicon containing 14,000 English\nwords with VAD scores for Valence, Arousal, and Dominance, that can be used to train the classi\ufb01er, similarly to the\napproach by Mantyla et al. [20]. In [20] they extracted the valence-arousal-dominance (VAD) metrics from 700,000\nJira issue reports containing over 2,000,000 comments and showed that issue reports of different type (e.g., feature\nrequest vs bug) had a fair variation of valence. In contrast, an increase in issue priority typically increased arousal. Finally, sentiment is measured using the BERT classi\ufb01er explained in 2.3 trained with the public dataset used in similar\nstudies [3,4]. The algorithm extracts the sentiment polarity expressed in short texts in three levels: positive (1), neutral\n(0) and negative (-1) sentiment. Our analysis focuses on three main classes of affect metrics: emotions (love, joy, anger, sadness), VAD (valence,\narousal, dominance) and Sentiment. As we specify in Section 2.3, we use a tailor-made tool to extract it from the text\nof the comments for each affect metric class. Once numerical values of the affect metrics are computed for all comments (as shown in the example in Tables 7 and\n8), we consider the comments timestamps (i.e. dates when the comments was posted) to build the corresponding social\nmedia time series.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 370,
      "chunk_id": "chunk_20"
    },
    {
      "text": "Once numerical values of the affect metrics are computed for all comments (as shown in the example in Tables 7 and\n8), we consider the comments timestamps (i.e. dates when the comments was posted) to build the corresponding social\nmedia time series. The affect time series are constructed aggregating sentiment and emotions of multiple comments\non each hour and day depending on the time resolution considered (hourly and daily). For a given social media indicator, e.g. anger, and for a speci\ufb01c time resolution, we construct the time series by\naveraging the values of the affect metric over all comments posted on the same day. In Table 12 and 13 we report in more details the summary statistics of the social indicators\u2019 time series for both\ncryptocurrencies respectively. We also report in Figure 7 and 8 the time series for all social media indicators for\nBitcoin and Ethereum, respectively\n2.3.2\nMeasuring Affects Metrics on Reddit\nThe social media platform Reddit is an American social news aggregation, web content rating, and discussion website\nthat reaches about 8 billion page views per month. It is a top-rated social network in English-speaking countries,\nespecially Canada and the United States. Almost all the messages present are written in English, while the minority,\nare in Spanish, Italian, French and German. Reddit is built over multiple subreddits, where each subreddit is dedicated to discussing a particular subject. Therefore,\nthere are speci\ufb01c subreddits related to major cryptocurrency projects. For each cryptocurrency in this work, two\nsubreddits are analysed, one technical and one trading related. In Tab. 6 the considered subreddits. are shown.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 349,
      "chunk_id": "chunk_21"
    },
    {
      "text": "In Tab. 6 the considered subreddits. are shown. For\neach subreddit, we fetched all comments from January 2017 to January 2021.\n5ITS are software platform used by open source communities and private software companies to manage the development\nprocess.\n6The F1 score tests the accuracy of a classi\ufb01er. It is calculated as the harmonic mean of precision and recall.\n11\nA PREPRINT - FEBRUARY 18, 2021\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.141\n2.273\n3.321\n3.365\n0.0729\n0.056\n0.227\n0.109\nstd\n0.774429\n2.953897\n4.324653\n4.376877\n0.293435\n0.248373\n0.566562\n0.393479\nmin\n-11\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n1.27\n1.85\n1.87\n0\n0\n0\n0\n75%\n0\n3.29\n4.8\n4.87\n0\n0\n0\n0\nmax\n15\n38.88\n60.78\n62.28\n6\n4\n11\n17\nTable 12: Summary statistics of Github affect metrics for Bitcoin.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 321,
      "chunk_id": "chunk_22"
    },
    {
      "text": "It is calculated as the harmonic mean of precision and recall.\n11\nA PREPRINT - FEBRUARY 18, 2021\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.141\n2.273\n3.321\n3.365\n0.0729\n0.056\n0.227\n0.109\nstd\n0.774429\n2.953897\n4.324653\n4.376877\n0.293435\n0.248373\n0.566562\n0.393479\nmin\n-11\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n1.27\n1.85\n1.87\n0\n0\n0\n0\n75%\n0\n3.29\n4.8\n4.87\n0\n0\n0\n0\nmax\n15\n38.88\n60.78\n62.28\n6\n4\n11\n17\nTable 12: Summary statistics of Github affect metrics for Bitcoin. Figure 7: Social Media Indicators time series extracted from Github Bitcoin developers comments.\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.0842\n0.7934\n1.1405\n1.147\n0.0182\n0.0761\n0.0961\n0.0356\nstd\n0.6754\n1.7082\n2.4653\n2.477\n0.1407\n0.5284\n0.3570\n0.2052\nmin\n-4\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n0\n0\n0\n0\n0\n0\n0\n75%\n0\n1.08\n1.54\n1.62\n0\n0\n0\n0\nmax\n31\n35.19\n52.5\n54.35\n3\n31\n6\n4\nTable 13: Summary Statistics of Github Social Media Indicators for Ethereum.\n12\nA PREPRINT - FEBRUARY 18, 2021\nFigure 8: Social Media Indicators time series extracted for Github Ethereum developers comments.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 523,
      "chunk_id": "chunk_23"
    },
    {
      "text": "Figure 7: Social Media Indicators time series extracted from Github Bitcoin developers comments.\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.0842\n0.7934\n1.1405\n1.147\n0.0182\n0.0761\n0.0961\n0.0356\nstd\n0.6754\n1.7082\n2.4653\n2.477\n0.1407\n0.5284\n0.3570\n0.2052\nmin\n-4\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n0\n0\n0\n0\n0\n0\n0\n75%\n0\n1.08\n1.54\n1.62\n0\n0\n0\n0\nmax\n31\n35.19\n52.5\n54.35\n3\n31\n6\n4\nTable 13: Summary Statistics of Github Social Media Indicators for Ethereum.\n12\nA PREPRINT - FEBRUARY 18, 2021\nFigure 8: Social Media Indicators time series extracted for Github Ethereum developers comments. For emotion detection we use the BERT classi\ufb01er explained in 2.3 trained with the public Github\u2019s emotion dataset\ndeveloped by Ortu et al. [24] and extended by Murgia et al. [23].",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 324,
      "chunk_id": "chunk_24"
    },
    {
      "text": "For emotion detection we use the BERT classi\ufb01er explained in 2.3 trained with the public Github\u2019s emotion dataset\ndeveloped by Ortu et al. [24] and extended by Murgia et al. [23]. This dataset is particularly suited for our analysis, as\nalready explained in the previous section.\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n1.8582\n8.6046\n12.0466\n11.7412\n0.6492\n0.2509\n0.5579\n2.2197\nstd\n4.3498\n17.3895\n24.4038\n23.7624\n1.7040\n0.8278\n1.4223\n4.7780\nmin\n-9\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n1.09\n1.54\n1.51\n0\n0\n0\n0\n75%\n2\n10.25\n14.22\n13.88\n1\n0\n0\n2\nmax\n101\n492.99\n680.41\n662.39\n42\n27\n34\n133\nTable 14: Summary Statistics of Reddit Social Media Indicators for subreddit r/Bitcoin.\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.0842\n0.7934\n1.1405\n1.147\n0.0182\n0.0761\n0.0961\n0.0356\nstd\n0.6754\n1.7082\n2.4653\n2.477\n0.1407\n0.5284\n0.3570\n0.2052\nmin\n-4\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n0\n0\n0\n0\n0\n0\n0\n75%\n0\n1.08\n1.54\n1.62\n0\n0\n0\n0\nmax\n31\n35.19\n52.5\n54.35\n3\n31\n6\n4\nTable 15: Summary Statistics of Reddit Social Media Indicators for subreddit r/Ethereum.\n13\nA PREPRINT - FEBRUARY 18, 2021\nFigure 9: Social Media Indicators time series extracted for Reddit for subreddit r/Bitcoin.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 564,
      "chunk_id": "chunk_25"
    },
    {
      "text": "This dataset is particularly suited for our analysis, as\nalready explained in the previous section.\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n1.8582\n8.6046\n12.0466\n11.7412\n0.6492\n0.2509\n0.5579\n2.2197\nstd\n4.3498\n17.3895\n24.4038\n23.7624\n1.7040\n0.8278\n1.4223\n4.7780\nmin\n-9\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n1.09\n1.54\n1.51\n0\n0\n0\n0\n75%\n2\n10.25\n14.22\n13.88\n1\n0\n0\n2\nmax\n101\n492.99\n680.41\n662.39\n42\n27\n34\n133\nTable 14: Summary Statistics of Reddit Social Media Indicators for subreddit r/Bitcoin.\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.0842\n0.7934\n1.1405\n1.147\n0.0182\n0.0761\n0.0961\n0.0356\nstd\n0.6754\n1.7082\n2.4653\n2.477\n0.1407\n0.5284\n0.3570\n0.2052\nmin\n-4\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n0\n0\n0\n0\n0\n0\n0\n75%\n0\n1.08\n1.54\n1.62\n0\n0\n0\n0\nmax\n31\n35.19\n52.5\n54.35\n3\n31\n6\n4\nTable 15: Summary Statistics of Reddit Social Media Indicators for subreddit r/Ethereum.\n13\nA PREPRINT - FEBRUARY 18, 2021\nFigure 9: Social Media Indicators time series extracted for Reddit for subreddit r/Bitcoin. Figure 10: Social Media Indicators time series extracted for Reddit for subreddit r/Ethereum.\n14\nA PREPRINT - FEBRUARY 18, 2021\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.9264\n4.0327\n5.6617\n5.5688\n0.2419\n0.1059\n0.3383\n1.0095\nstd\n2.7650\n10.7106\n14.9023\n14.6243\n0.8616\n0.4553\n1.0275\n2.8993\nmin\n-9\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n0\n0\n0\n0\n0\n0\n0\n75%\n0\n2.32\n3.2925\n3.26\n0\n0\n0\n0\nmax\n52\n245.32\n332.84\n329.4\n34\n15\n22\n88\nTable 16: Summary Statistics of Reddit Social Media Indicators for subreddit r/Bitcoinmakets.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 780,
      "chunk_id": "chunk_26"
    },
    {
      "text": "Figure 10: Social Media Indicators time series extracted for Reddit for subreddit r/Ethereum.\n14\nA PREPRINT - FEBRUARY 18, 2021\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.9264\n4.0327\n5.6617\n5.5688\n0.2419\n0.1059\n0.3383\n1.0095\nstd\n2.7650\n10.7106\n14.9023\n14.6243\n0.8616\n0.4553\n1.0275\n2.8993\nmin\n-9\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n0\n0\n0\n0\n0\n0\n0\n75%\n0\n2.32\n3.2925\n3.26\n0\n0\n0\n0\nmax\n52\n245.32\n332.84\n329.4\n34\n15\n22\n88\nTable 16: Summary Statistics of Reddit Social Media Indicators for subreddit r/Bitcoinmakets. Figure 11: Social Media Indicators time series extracted for Reddit for subreddit r/Bitcoinmakets.\nsentiment\narousal\nvalence\ndominance\njoy\nlove\nsadness\nanger\nmean\n0.8150\n2.8479\n4.0716\n4.0046\n0.2123\n0.0855\n0.2072\n0.5983\nstd\n2.1528\n6.1395\n8.7652\n8.6220\n0.6807\n0.3959\n0.6641\n1.577632\nmin\n-6\n0\n0\n0\n0\n0\n0\n0\n25%\n0\n0\n0\n0\n0\n0\n0\n0\n50%\n0\n0\n0\n0\n0\n0\n0\n0\n75%\n1\n3.25\n4.65\n4.6\n0\n0\n0\n1\nmax\n62\n138.39\n207.38\n191.95\n28\n23\n13\n37\nTable 17: Summary Statistics of Reddit Social Media Indicators for subreddit r/Ethtraders.\n15",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.3 Social Media Indicators Evaluation Through Deep Learning",
        "level": 2,
        "page_start": 7
      },
      "token_count": 518,
      "chunk_id": "chunk_27"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nFigure 12: Social Media Indicators time series extracted for Reddit for subreddit r/Ethtraders. The classi\ufb01er can detect love, anger, joy and sadness with an F1 score7 close to 0.89 for all of them. For VAD metrics\nwe used the same approach in 2.3.1 while for sentiment we used previous approach with BERT deep learning algorithm\ntrained with a public golden dataset for Reddit comments available in the biggest and well known web platform for\nsharing datasets Kaggle.com8. Tables 14 and 16 and Figures 9 and 11 show statistics and time series for the two Bitcoin\u2019s subreddits while Tables 17\nand 15 and Figures 10 and 12 show statistics and time series for the two Ethereum\u2019s subreddits.\n2.4\nPrice Movement Classi\ufb01cation\nThe target variable is a binary variable with two unique classes listed below.\n\u2022 Upward movements: This class, labeled with up and encoded with 1, represents the condition of increasing\nprices.\n\u2022 Downward movements: This class, labeled down and encoded with 0, represents the condition of falling\nprices. Figure 13 shows the class distribution and the dataset for hourly and daily frequency, highlighting that we are dealing\nwith fairly balanced classi\ufb01cation problems in the case of hourly frequency and slightly unbalanced in the daily\nfrequency case. Table 18 shows the details about the instances of classes down and up, with 48, 5% and 51.5% respectively for Bitcoin\nand 49, 8% and 50, 2% for Ethereum with and hourly frequency. For daily frequency we have 44, 8% and 55.2% for\nBitcoin and 48, 5% and 51, 5% for Ethereum of down and up class instances.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.5 Time Series Processing",
        "level": 2,
        "page_start": 16
      },
      "token_count": 400,
      "chunk_id": "chunk_28"
    },
    {
      "text": "For daily frequency we have 44, 8% and 55.2% for\nBitcoin and 48, 5% and 51, 5% for Ethereum of down and up class instances. For Bitcoin daily frequency we have a\nslightly unbalanced distribution toward up classes, in this case we will consider f1-score along with accuracy to asses\nthe model performance.\n2.5\nTime Series Processing\nSince we are using a supervised learning problem, we prepare our data to have a vector of x inputs and an y output\nwith temporal dependence. In this case, the input vector x is called regressor. The x inputs include the model\u2019s\n7The F1 score tests the accuracy of a classi\ufb01er and it is calculated as the harmonic mean of precision and recall.\n8https://www.kaggle.com/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n16\nA PREPRINT - FEBRUARY 18, 2021\n(a) Bitcoin Hourly\n(b) Ethereum Hourly\n(c) Bitcoin Daily\n(d) Ethereum Daily\nFigure 13: Hourly (13a,13b) and Daily (13c,13d) Price Movement Classes Distribution. Frequency\nCryptocurrency\nClass\nCounts\nPercentage\nHourly\nBitcoin\nup\n17246\n48,5%\ndown\n18271\n51,5%\nEthereum\nup\n16844\n49,8%\ndown\n16956\n50,2%\nDaily\nBitcoin\nup\n665\n44,8%\ndown\n817\n55,8%\nEthereum\nup\n684\n48,5%\ndown\n727\n51,5%\nTable 18: Class instances counts and percentages for Bitcoin and Ethereum at an hourly or daily frequency.\n17",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "2.5 Time Series Processing",
        "level": 2,
        "page_start": 16
      },
      "token_count": 372,
      "chunk_id": "chunk_29"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\npredictors, i.e. one or several values from the past, the so-called lagged values. Inputs correspond to the values of\nthe selected features discussed in the previous sections. The target variable y is a binary variable, which can be either\n0 or 1. The 0 (down) instance represents downward price movements. A 0 instance at time t is obtained when the\ndifference between the close price at time t and the open price at time t + 1 is less than or equal to 0. The 1 (up)\ninstance represents upward price movements, i.e. a rising price condition. A 1 instance is obtained when the difference\nbetween the close price at time t and the open price at next time step t + 1 is greater than 0. We considered two time\nseries models:\n\u2022 Restricted: the input vector x consists of only technical indicators (open, close, high, low, volume).\n\u2022 Unrestricted: the input vector x consists of technical, trading and social indicators. For both the restricted and unrestricted model we used 1 lagged value for each indicator. The purpose of this distinction\nis to ascertain and quantify whether the addition of trading and social indicators to the regressor vector leads to an\neffective improvement in the Bitcoin and Ethereum price changes classi\ufb01cation.\n3\nMethodology\nThis section describes the deep learning algorithms considered in our analysis, followed by a discussion on the \ufb01ne\ntuning of the hyper-parameters.\n3.1\nMultilayer Perceptron\nA multilayer perceptron (MLP) is a class of feed-forward arti\ufb01cial neural networks (ANNs), characterised by multiple\nlayers of perceptrons and a typical activation function. Figure 14: Scheme of the Multilayer Perceptron architecture.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.1 Multilayer Perceptron",
        "level": 2,
        "page_start": 18
      },
      "token_count": 390,
      "chunk_id": "chunk_30"
    },
    {
      "text": "Figure 14: Scheme of the Multilayer Perceptron architecture. The most common activation function are:\ny(vi) = tanh(vi) and y(vi) = (1 + e\u2212vi)\u22121 ,\n(1)\nwhere vi is the weighted vector of inputs.\n18",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.1 Multilayer Perceptron",
        "level": 2,
        "page_start": 18
      },
      "token_count": 61,
      "chunk_id": "chunk_31"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nWhen composed of a single hidden layer as in Figure 14, MLPs are called \u201cvanilla\u201d neural networks (in jargon and for\npractical use). In general, MLPs refer to neural network architectures with two or more hidden layers. A MLP comprises three main node categories: input layer nodes, hidden layer nodes and output layer nodes. All nodes\nof the neural network are perceptrons that use a nonlinear activation function, except for the input nodes. MLP differs\nfrom a linear perceptron because of its multiple layers and nonlinear activation functions. In general, MLP Neural networks are resilient to noise and can also support learning and inference when values are\nmissing. Neural networks do not make strong assumptions about the mapping function and readily learn both linear\nand nonlinear relationships. An arbitrary number of input features can be speci\ufb01ed, providing direct support for\nmultidimensional forecasting. An arbitrary number of output values can be speci\ufb01ed, providing direct support for\nmulti-step and even multivariate forecasting. For these reasons, MLP neural networks may be particularly useful for\ntime series forecasting. In recent developments of deep learning techniques, the recti\ufb01er linear unit (ReLU), a piecewise linear function,\nis frequently used to overcome numerical problems associated with sigmoid functions. Examples of ReLU are the\nhyperbolic tangent varing between -1 and 1, or the logistic function between 0 and 1. The output of the i-th node\n(neuron) here is yi, and the weighted sum of the input connections is vi. By including the recti\ufb01er and softmax functions, alternative activation functions have been developed. Radial basis\nfunctions include more advanced activation functions (used in radial basis networks, another class of supervised neural\nnetwork models).",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.2 Long Short Term Memory",
        "level": 2,
        "page_start": 19
      },
      "token_count": 381,
      "chunk_id": "chunk_32"
    },
    {
      "text": "Radial basis\nfunctions include more advanced activation functions (used in radial basis networks, another class of supervised neural\nnetwork models). Since MLPs are fully connected architectures, each node in one layer connects with a speci\ufb01c weight wi,j to every\nnode in the following layer. The neural network is trained using a supervised method called back-propagation and\nan optimiser method (the Stochastic Gradient Descent is the \ufb01rst and widely used method). After data is processed,\nlearning occurs in the perceptron by adjusting the connection weights, depending on the amount of error in the output\nrelative to the expected result. Back-propagation in the perceptron is a generalisation of the least mean squares (LMS)\nalgorithm. When the nth training sample is presented to the input layer, the amount of error in the output node j is ej(n) =\ndj(n) \u2212yj(n), where d is the predicted value and y is the actual value that the perceptron should generate. The\nback-propagation method then adjusts the node weights to minimise the entire output error provided by Eq. (2):\n\u01eb(n) = 1\n2\nP\nj e2\nj(n) .\n(2)\nThe adjustment of each node\u2019s weight is further computed using the gradient descent in Eq. (3), where yi is the output\nof the previous neuron and \u03b7 is the learning rate:\n\u2206wj,i(n) = \u2212\u03b7 \u2202\u01eb(n)\n\u2202vj(n)yi(n) .\n(3)\nThe parameter \u03b7 is commonly set as a trade-off between the weights\u2019 convergence to a response and the oscillations\naround the response.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.2 Long Short Term Memory",
        "level": 2,
        "page_start": 19
      },
      "token_count": 343,
      "chunk_id": "chunk_33"
    },
    {
      "text": "The\nback-propagation method then adjusts the node weights to minimise the entire output error provided by Eq. (2):\n\u01eb(n) = 1\n2\nP\nj e2\nj(n) .\n(2)\nThe adjustment of each node\u2019s weight is further computed using the gradient descent in Eq. (3), where yi is the output\nof the previous neuron and \u03b7 is the learning rate:\n\u2206wj,i(n) = \u2212\u03b7 \u2202\u01eb(n)\n\u2202vj(n)yi(n) .\n(3)\nThe parameter \u03b7 is commonly set as a trade-off between the weights\u2019 convergence to a response and the oscillations\naround the response. The induced local \ufb01eld vj varies and one can compute its derivative:\n\u2212\u2202\u01eb(n)\n\u2202vj(n) = ej(n)\u03c6\u2032(vj(n))\n\u2212\u2202\u01eb(n)\n\u2202vj(n) = \u03c6\u2032(vj(n)) P\nk \u2212\u2202\u01eb(n)\n\u2202vk(n)wk,j(n) ,\n(4)\nwhere \u03c6\u2032 is the derivative of the activation function described above, which itself does not vary. The analysis is\nmore dif\ufb01cult when modifying the weights of a hidden node, but it can be shown that the relevant quantity is the one\nshowed in Eq. (4). This algorithm represents a back-propagation of the activation function as Eq. (4) depends on\nthe adjustment of the weights of the kth layer, which represent the output layer and this adjustment in turn changes\ndepending on the derivative of the activation functions of the hidden layer weights.\n3.2\nLong Short Term Memory\nLong Short-Term Memory networks are a specialised version of Recurrent Neural Network (RNN) able to capture\nlong-term dependencies in a sequence of data.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.2 Long Short Term Memory",
        "level": 2,
        "page_start": 19
      },
      "token_count": 376,
      "chunk_id": "chunk_34"
    },
    {
      "text": "This algorithm represents a back-propagation of the activation function as Eq. (4) depends on\nthe adjustment of the weights of the kth layer, which represent the output layer and this adjustment in turn changes\ndepending on the derivative of the activation functions of the hidden layer weights.\n3.2\nLong Short Term Memory\nLong Short-Term Memory networks are a specialised version of Recurrent Neural Network (RNN) able to capture\nlong-term dependencies in a sequence of data. RNNs are a type of arti\ufb01cial neural networks with a particular topology\nspecialised in the identi\ufb01cation of patterns in different types of data sequences: natural language, DNA sequences,\nhandwriting, word sequences, or numerical time series data streams from sensors and \ufb01nancial markets [12] for ex-\nample. Classical recurrent neural networks have a signi\ufb01cant disadvantage related to their inability to address long\n19\nA PREPRINT - FEBRUARY 18, 2021\nsequences and capture long-term dependencies. RNNs could instead be used only for short sequences with short-term\nmemory dependencies. LSTM were introduced to address the long-term memory problem and are derived directly\nfrom RNN to capture long-term dependencies. An LSTM neural network is organised in units called cells, performing\ntransformations of the input sequence transformation by applying a series operations. An internal state variable is\nretained by an LSTM cell when forwarded from one cell to the next and is updated by the so-called Operation Gates\n(forget gate, input gate, output gate) as shown in Figure 16. All three gates have different and independent weights\nand biases, so the network can learn how much of the previous output and current input to maintain and how much of\nthe internal state to pass to the output. Such gates control how much of the internal state is transmitted to the output\nand operate similarly to other gates.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.2 Long Short Term Memory",
        "level": 2,
        "page_start": 19
      },
      "token_count": 394,
      "chunk_id": "chunk_35"
    },
    {
      "text": "Such gates control how much of the internal state is transmitted to the output\nand operate similarly to other gates. An LSTM cell unit consists of:\n1. A cell state: this state brings information along the entire sequence and represents the memory of the network.\n2. A forget gate: it \ufb01lters the relevant information to be kept from previous time steps.\n3. An input gate: it decides what information is relevant to be added add from the current time step.\n4. An output gate: it controls the amount of output at the current time step. Figure 15: LSTM Cell Gate. The \ufb01rst step is the forget gate. This gate takes as input past or lagged values and decides how much of the past\ninformation should be forgotten and how much should be saved. The input from the previous hidden state and the\ncurrent input are transferred through the sigmoid function to the output gate. An output is close to 0 when that piece\nof information can be forgotten, while it is close to 1 when that piece of information is to be saved, as follows:\nf(t) = \u03c3(x(t) \u2217Uf + h(t + 1) \u2217Wf) .\n(5)\nThe matrices Wf and Uf contain, respectively, the weights of the input and recurrent connections. The subscript f\ncan be either indicate the forget gate. xt represents the input vector to the LSTM and ht+1 the hidden state vector or\noutput vector of the LSTM unit. The second gate is the input gate. At this stage the cell state is updated. The previous hidden state and the current\ninput are initially presented as inputs to a sigmoid activation function (the closer the value is to 1, the more relevant\nthe input is). To boost the network-tuning, it also passes the hidden state and current input to the tanh function to\ncompress values between \u22121 and 1.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.2 Long Short Term Memory",
        "level": 2,
        "page_start": 19
      },
      "token_count": 391,
      "chunk_id": "chunk_36"
    },
    {
      "text": "To boost the network-tuning, it also passes the hidden state and current input to the tanh function to\ncompress values between \u22121 and 1. Then the output of the tanh and of the sigmoid are multiplied element by element\n(in the formula below the symbol \u2217indicates the multiplication element by element of two matrices). The sigmoid\noutput, in Equation 6 determines the information that is important to keep from the tanh output:\ni1(t) = \u03c3(x(t) \u2217Ui + h(t + 1) \u2217Wi) ,\ni2(t) = tanh(x(t) \u2217Ug + h(t + 1) \u2217Wg) ,\ni(t) = i1(t) \u2217i2(t) .\n(6)\n20",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.2 Long Short Term Memory",
        "level": 2,
        "page_start": 19
      },
      "token_count": 161,
      "chunk_id": "chunk_37"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nThe cell state can be determined after the input gate activation. Next, the cell state of the previous time step is\nmultiplied element-by-element by the forget gate output. This leads to dismissing values when multiplied by values\nclose to 0 in the cell state. The input gate output is added element-wise to the cell state. The new cell state in Equation\n7 is the output:\nC(t) = \u03c3(f(t) \u2217C(t \u22121) + it) .\n(7)\nThe \ufb01nal gate is the output gate, which speci\ufb01es the next hidden state\u2019s value, which includes a certain amount of\nprevious input information. Here the current input and the previous hidden state are summed up and forwarded to the\nsigmoid function. The new cell state is then transferred to the tanh function. At the end, the tanh output with the\nsigmoid output is multiplied to determine which information the hidden state can carry. The output is a hidden new\nstate. The new cell state and the new hidden state are then shifted to the next stage by Equations 8:\no(t) = \u03c3(x(t) \u2217Uo + h(t \u22121) \u2217Wo) ,\nh(t) = tanh(Ct) \u2217o(t) .\n(8)\nTo conduct this analysis, we used the Keras framework [7] for deep learning. Our model consists of one stacked LSTM\nlayer and a densely connected output layer with one neuron.\n3.3\nAttention Mechanism Neural Network\nThe Attention Function is one of the key aspects of Deep Learning algorithms, an extension of the Encoder-Decoder\nParadigm, developed to improve the output on long input sequences. Figure 16 shows the key idea behind the AMNN,\nwhich is to allow the decoder, during decoding, to access encoder information selectively.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.3 Attention Mechanism Neural Network",
        "level": 2,
        "page_start": 21
      },
      "token_count": 391,
      "chunk_id": "chunk_38"
    },
    {
      "text": "Figure 16 shows the key idea behind the AMNN,\nwhich is to allow the decoder, during decoding, to access encoder information selectively. This is achieved by creating\na new context vector for each decoder step, computing it according to the previous hidden state as well as all encoder\u2019s\nhidden states, assigning them trainable weights. In this way, the Attention Technique gives the input series a different\npriority and pays more attention to the most important inputs. Figure 16: Attention Mechanism Neural Network. The encoder operation is very similar to the Encoder-Decoder hybrid operation itself. The representation of each input\nsequence is determined at each time step, as a function of the previous time step\u2019s hidden state and the current input.\n21",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.3 Attention Mechanism Neural Network",
        "level": 2,
        "page_start": 21
      },
      "token_count": 147,
      "chunk_id": "chunk_39"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nThe \ufb01nal hidden state includes all encoded information from the previous hidden representations and the previous\ninputs. The key distinction between the Attention mechanism and the Encoder-Decoder model is that with each decoder step\nt, a new background vector c(t) is computed. We proceed as follows to measure the context vector c(t) for time step t. First of all, the so-called alignment scores e(j, t) are calculated with the weighted sum in Eq. (9) for each combination\nof the time step j of the encoder and time step t of the decoder:\ne(j, t) = Va \u2217tanh(Ua \u2217s(t \u22121) + Wa \u2217h(j)) .\n(9)\nWa, Ua and Va are learning weights in this formula, which are referred to as attention weights. The Wa weights are\nlinked to the encoder\u2019s hidden states, the Ua weights are linked to the decoder\u2019s hidden states, and the Va weights\ndetermine the function that computes the alignment score. The scores e(j, t) are normalized at each time step t using\nthe softmax function over the time stages of the encoder j, obtaining the attention weights \u03b1(j, t) as follows:\n\u03b1(j, t) =\nexp(e(j, t))\nPM\nj=1 exp(e(j, t))\n.\n(10)\nThe importance of the input at time j is represented by the attention weight \u03b1(j, t) for decoding the output of time t. The context vector c(t) is estimated according to the attention weights as the weighted sum of all hidden values of the\nencoder as follows:\nc(j, t) =\nM\nX\nj=1\n\u03b1(j, t)h(j) .\n(11)\nAccording to this method, the so-called attention function is triggered by the contextual data vector, weighting more\nthe most important inputs.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.4 Convolutional Neural Network",
        "level": 2,
        "page_start": 22
      },
      "token_count": 398,
      "chunk_id": "chunk_40"
    },
    {
      "text": "The context vector c(t) is estimated according to the attention weights as the weighted sum of all hidden values of the\nencoder as follows:\nc(j, t) =\nM\nX\nj=1\n\u03b1(j, t)h(j) .\n(11)\nAccording to this method, the so-called attention function is triggered by the contextual data vector, weighting more\nthe most important inputs. The contextual vector c(t) is now forwarded to the decoder to calculate the probability distribution for the next possible\noutput. This decoding operation refers to all the time steps present in the input. The current hidden state s(t) is then\ncalculated according to the recurring unit function, taking as input the contextual vector c(t), the hidden state s(t \u22121)\nand the output \u02c6y(t \u22121) according to the equation:\ns(j, t) = f(s(t \u22121), \u02c6y(t \u22121), c(t)) .\n(12)\nUsing this function, the model can identify the relationship between the different parts of the input sequence and the\ncorresponding parts of the output sequence. The softmax function is used to calculate the output of the decoder in the\nweighted hidden state at each time t:\n\u02c6y(t) = softmax(Vs(t)) .\n(13)\nConcerning the LSTM, the Attention mechanism provides better results with long input sequences, due to the attention\nweights. In this study, we speci\ufb01cally use a Multivariate Attention LSTM with Fully Convolutional Network (MALSTM-FCN)\nproposed by Fazle et al. [14, 15]. Figure 17 shows the architecture for the MALSTM-FCN including the number of\nneurons per layer. The input sequence goes in parallel to a fully convolutional layers and Attention LSTM layers,\nand is concatenated and passed to the output layer via a softmax activation function for binary classi\ufb01cation.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.4 Convolutional Neural Network",
        "level": 2,
        "page_start": 22
      },
      "token_count": 395,
      "chunk_id": "chunk_41"
    },
    {
      "text": "The input sequence goes in parallel to a fully convolutional layers and Attention LSTM layers,\nand is concatenated and passed to the output layer via a softmax activation function for binary classi\ufb01cation. The\nfully convolutional block contains three temporal convolutional blocks of 128, 256 and 256 neurons respectively, used\nas feature extractors. Each convolutional layer is succeeded by batch normalisation, before the concatenation. The\ndimension shuf\ufb02e transposes the temporal dimension of the input data, so that the LSTM is given the global temporal\ninformation of each variable at once. As a result, the dimension shuf\ufb02e operation reduces the computation time of\ntraining and inference without losing accuracy for time series classi\ufb01cation problems [15].\n3.4\nConvolutional Neural Network\nA Convolutional Neural Network (CNN) is a speci\ufb01c class of neural networks most commonly used for deep learning\napplications concerning image processing, image classi\ufb01cation, natural language processing and \ufb01nancial time series\nanalysis [6]. The most critical part of the CNN architecture is the convolutional layer. This layer performs a mathematical operation\ncalled convolution. In this context, a convolution is a linear operation that involves a multiplication between a matrix\n22",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.4 Convolutional Neural Network",
        "level": 2,
        "page_start": 22
      },
      "token_count": 266,
      "chunk_id": "chunk_42"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nFigure 17: Attention LSTM cells to construct the MALSTM-FCN architecture [15].\nof input data and a two-dimensional array of weights, known as a \ufb01lter. These networks use convolution operation in\nat least one of their layers. Figure 18: Convolutional Neural Network for time series forecasting. Convolutional neural networks share a similar architecture with traditional neural networks, including an input and\nan output layer and multiple hidden layers. The main feature of a CNN is that its hidden layers typically consist\nof convolutional layers that perform the operations described above. Figure 18 depicts the general architecture of\nCNNs for time series analysis. We use a one-dimensional convolutional layer instead of the usual two-dimensional\nconvolutional layer typical in image processing tasks. This \ufb01rst layer is then normalised with a polling layer and later\n\ufb02attened so that the output layer can process the whole time series at each step t. In this case, many one-dimensional\nconvolution layers can be combined in a deep learning network. For the CNN implementation, we used the Keras framework [7] for deep learning. Our model consists of two or more\nstacked 1-dimensional CNN layers, one densely connected layer with N neurons for polling, one densely connected\nlayer with N neurons for \ufb02attering, and \ufb01nally the densely connected output layer with one neuron.\n3.5\nHyper-parameters tuning\nThe hyper-parameters tuning is a method for the optimisation of the hyper-parameters of a given algorithm. It is\nused to identify the best con\ufb01guration of the hyper-parameters that would allow the algorithm to achieve the best\nperformance, evaluated with respect to a speci\ufb01c prediction error.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.5 Hyper-parameters tuning",
        "level": 2,
        "page_start": 23
      },
      "token_count": 377,
      "chunk_id": "chunk_43"
    },
    {
      "text": "It is\nused to identify the best con\ufb01guration of the hyper-parameters that would allow the algorithm to achieve the best\nperformance, evaluated with respect to a speci\ufb01c prediction error. For each algorithm, the hyper-parameters to be\noptimised are selected, and for each hyper-parameter an appropriate searching interval is de\ufb01ned, including all values\n23",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "3.5 Hyper-parameters tuning",
        "level": 2,
        "page_start": 23
      },
      "token_count": 79,
      "chunk_id": "chunk_44"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nto be tested. The algorithm is then \ufb01tted on a speci\ufb01c portion of the dataset with the \ufb01rst chosen hyper-parameter\ncon\ufb01guration. The \ufb01tted model is tested on a portion of data that has not been previously used during the training\nphase. This testing procedure returns a speci\ufb01c value for the chosen prediction error.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.1 Hyper-Parameters For The Restricted Model",
        "level": 2,
        "page_start": 24
      },
      "token_count": 95,
      "chunk_id": "chunk_45"
    },
    {
      "text": "This testing procedure returns a speci\ufb01c value for the chosen prediction error. Algorithm\nParameter\nSearching Interval\nMLP\nepochs\n100, 250, 500, 1000\nhidden layers\n1, 2, 3, 4, 5\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\nrelu, tanh, softmax\nneurons\n16, 32, 64, 128, 256\nLSTM\nepochs\n100, 250, 500, 1000\nhidden layers\n1, 2, 3, 4, 5\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\nrelu, tanh\nneurons\n16, 32, 64, 128, 256\nMALSTM-FCN\nepochs\n100, 250, 500, 1000\nhidden layers\n-\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\n-\nneurons\n-\nCNN\nepochs\n100, 250, 500, 1000\nhidden layers\n1, 2, 3, 4, 5\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\nrelu, tanh, softmax\nneurons\n16, 32, 64, 128, 256\nTable 19: hHyper-parameter searching intervals for different neural network architectures.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.1 Hyper-Parameters For The Restricted Model",
        "level": 2,
        "page_start": 24
      },
      "token_count": 375,
      "chunk_id": "chunk_46"
    },
    {
      "text": "Algorithm\nParameter\nSearching Interval\nMLP\nepochs\n100, 250, 500, 1000\nhidden layers\n1, 2, 3, 4, 5\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\nrelu, tanh, softmax\nneurons\n16, 32, 64, 128, 256\nLSTM\nepochs\n100, 250, 500, 1000\nhidden layers\n1, 2, 3, 4, 5\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\nrelu, tanh\nneurons\n16, 32, 64, 128, 256\nMALSTM-FCN\nepochs\n100, 250, 500, 1000\nhidden layers\n-\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\n-\nneurons\n-\nCNN\nepochs\n100, 250, 500, 1000\nhidden layers\n1, 2, 3, 4, 5\nbatch size\n32, 64, 128, 256, 512\noptimizer\nadam, Nadam, Adamax, RMSprop, SGD\nactivation\nrelu, tanh, softmax\nneurons\n16, 32, 64, 128, 256\nTable 19: hHyper-parameter searching intervals for different neural network architectures. The optimisation procedure via the Grid Search procedure [19] ends when all possible combinations of hyper-\nparameter values have been tested.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.1 Hyper-Parameters For The Restricted Model",
        "level": 2,
        "page_start": 24
      },
      "token_count": 384,
      "chunk_id": "chunk_47"
    },
    {
      "text": "The optimisation procedure via the Grid Search procedure [19] ends when all possible combinations of hyper-\nparameter values have been tested. The hyper-parameter con\ufb01guration yielding the best performance in terms of\nthe selected prediction error is therefore chosen as the optimised con\ufb01guration. Table 19 show the hyper-parameters\u2019\nsearching intervals for each implemented algorithm. Since MALSTM-FCN is a deep neural network-speci\ufb01c archi-\ntecture, the number of layers, neurons per layer and activation function of each layer are already pre-speci\ufb01ed (as\nexplained in Section 3.3). To ensure the robustness of the hyper-parameter optimisation procedure, we use a model validation technique to assess\nhow the performance achieved by a given model will generalise to an independent dataset. This validation technique\ninvolves the partition of a data sample into a training set, used to \ufb01t the model, and a validation set used to validate\nthe \ufb01tted model and a test set to assess the \ufb01nal optimised generalisation power of the model. In our analysis, we\nimplemented the Boostrap Method [9] with 37.8% of out-of-bag samples and 10000 iterations to validate the \ufb01nal\nhyper-parameters.\n4\nEmpirical Evidence\nIn this section, we report and discuss the main results of the analysis. In particular, we discuss the outcome for both\nthe restricted and unrestricted models. These results are evaluated in terms of the standard classi\ufb01cation error metrics:\naccuracy, f1 score, precision and recall.\n4.1\nHyper-Parameters For The Restricted Model\nWe brie\ufb02y discuss here the \ufb01ne-tuning of the hyper-parameters of the four deep learning algorithm mentioned in\nSection 3.5 considering the hourly frequency resolution. Table 20 shows the best results obtained for the different\n24",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.1 Hyper-Parameters For The Restricted Model",
        "level": 2,
        "page_start": 24
      },
      "token_count": 400,
      "chunk_id": "chunk_48"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nneural networks models, using the Grid Search technique in terms of the classi\ufb01cation error metrics. The best identi\ufb01ed\nparameters with the related results obtained for the MALSTM-FNC and MLP models are reported in table 20. Algorithm\nParameter\nValues\nAccuracy (\u00b5 \u00b1 \u03c3)\nPrediction (\u00b5 \u00b1 \u03c3)\nRecall (\u00b5 \u00b1 \u03c3)\nf1-score (\u00b5 \u00b1 \u03c3)\nMLP\nepochs\n250\n0.537 \u00b1 0.029\n0.472 \u00b1 0.143\n0.511 \u00b1 0.025\n0.495 \u00b1 0.027\nhidden layers\n2\nbatch size\n256,\noptimizer\nNadam\nactivation\nrelu\nneurons\n128\nLSTM\nepochs\n250\n0.535 \u00b1 0.034\n0.456 \u00b1 0.200\n0.485 \u00b1 0.082\n0.503 \u00b1 0.285\nhidden layers\n2\nbatch size\n256,\noptimizer\nAdamx\nactivation\ntanh\nneurons\n256\nMALSTM-FCN\nepochs\n250\n0.542 \u00b1 0.034\n0.456 \u00b1 0.200\n0.485 \u00b1 0.082\n0.503 \u00b1 0.201\nhidden layers\n-\nbatch size\n256,\noptimizer\nAdamx\nactivation\n-\nneurons\n-\nCNN\nepochs\n250\n0.435 \u00b1 0.024\n0.486 \u00b1 0.210\n0.485 \u00b1 0.082\n0.453 \u00b1 0.265\nhidden layers\n2\nbatch size\n128,\noptimizer\nNadam\nactivation\ntanh\nneurons\n128\nTable 20: Restricted model - Neural networks optimal parameters.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.3 Results and Discussions",
        "level": 2,
        "page_start": 25
      },
      "token_count": 381,
      "chunk_id": "chunk_49"
    },
    {
      "text": "Algorithm\nParameter\nValues\nAccuracy (\u00b5 \u00b1 \u03c3)\nPrediction (\u00b5 \u00b1 \u03c3)\nRecall (\u00b5 \u00b1 \u03c3)\nf1-score (\u00b5 \u00b1 \u03c3)\nMLP\nepochs\n250\n0.537 \u00b1 0.029\n0.472 \u00b1 0.143\n0.511 \u00b1 0.025\n0.495 \u00b1 0.027\nhidden layers\n2\nbatch size\n256,\noptimizer\nNadam\nactivation\nrelu\nneurons\n128\nLSTM\nepochs\n250\n0.535 \u00b1 0.034\n0.456 \u00b1 0.200\n0.485 \u00b1 0.082\n0.503 \u00b1 0.285\nhidden layers\n2\nbatch size\n256,\noptimizer\nAdamx\nactivation\ntanh\nneurons\n256\nMALSTM-FCN\nepochs\n250\n0.542 \u00b1 0.034\n0.456 \u00b1 0.200\n0.485 \u00b1 0.082\n0.503 \u00b1 0.201\nhidden layers\n-\nbatch size\n256,\noptimizer\nAdamx\nactivation\n-\nneurons\n-\nCNN\nepochs\n250\n0.435 \u00b1 0.024\n0.486 \u00b1 0.210\n0.485 \u00b1 0.082\n0.453 \u00b1 0.265\nhidden layers\n2\nbatch size\n128,\noptimizer\nNadam\nactivation\ntanh\nneurons\n128\nTable 20: Restricted model - Neural networks optimal parameters. The neural network that achieved the best accuracy is MALSTM-FNC, with an average accuracy of 53.7% and a\nstandard deviation of 2.9%.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.3 Results and Discussions",
        "level": 2,
        "page_start": 25
      },
      "token_count": 349,
      "chunk_id": "chunk_50"
    },
    {
      "text": "The neural network that achieved the best accuracy is MALSTM-FNC, with an average accuracy of 53.7% and a\nstandard deviation of 2.9%. Among the implemented machine learning models, the one that achieved the best f1-score\nis again MALSTM-FNC, with an average accuracy of 54% and a standard deviation of 2.01% (the LSTM obtained the\nsame f1-score but we observe a higher variance).\n4.2\nHyper-Parameters For The Unrestricted Model\nTable 21 shows the best results obtained for the Neural Networks models, via the Grid Search technique with respect\nto the classi\ufb01cation error metrics. The best identi\ufb01ed parameters with the related results obtained for the CNN and\nLSTM models are reported in table 21. The results obtained for the unrestricted model highlight that the addition of trading and social media indicators to the\nmodel leads to an effective improvement in average accuracy, namely the prediction error. This result is consistent for\nall implemented algorithms, and this allows us to exclude that this result is a statistical \ufb02uctuation or that it may be an\nartefact of the particular classi\ufb01cation algorithm implemented. The best result obtained with the unrestricted model is\nachieved using the CNN model, with a mean accuracy of 87% and a standard deviation of 2.7%.\n4.3\nResults and Discussions\nTable 22 shows the results obtained using the four deep learning algorithms for the hourly frequency price movements\nclassi\ufb01cation task. This table presents the results for the restricted (upper part) and unrestricted (lower part) model. Firstly, it can be noted that for all four deep learning algorithms, the performance of the unrestricted model outperforms\nthe restricted model in terms of accuracy, precision, recall and F1 score.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.3 Results and Discussions",
        "level": 2,
        "page_start": 25
      },
      "token_count": 382,
      "chunk_id": "chunk_51"
    },
    {
      "text": "Firstly, it can be noted that for all four deep learning algorithms, the performance of the unrestricted model outperforms\nthe restricted model in terms of accuracy, precision, recall and F1 score. The accuracy ranges from 51% of for the\nrestricted MLP to 84% for CNNs and LSTMs. The fact that the result is consistent across all four classi\ufb01ers, further con\ufb01rm that is not due to statistical \ufb02uctuations,\nbut rather to the higher predictive of the unrestricted model. For Bitcoin, the highest performances are obtained using\nthe CNN architecture and for Ethereum by the LSTM.\n25",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "4.3 Results and Discussions",
        "level": 2,
        "page_start": 25
      },
      "token_count": 131,
      "chunk_id": "chunk_52"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nAlgorithm\nParameter\nValues\nAccuracy (\u00b5 \u00b1 \u03c3)\nPrediction (\u00b5 \u00b1 \u03c3)\nRecall (\u00b5 \u00b1 \u03c3)\nf1-score (\u00b5 \u00b1 \u03c3)\nMLP\nepochs\n500\n0.81 \u00b1 0.025\n0.984 \u00b1 0.180\n0.541 \u00b1 0.060\n0.698 \u00b1 0.908\nhidden layers\n3\nbatch size\n256\noptimizer\nNadam\nactivation\nrelu\nneurons\n128\nLSTM\nepochs\n1000\n0.86 \u00b1 0.027\n0.918 \u00b1 0.033\n0.873 \u00b1 0.228\n0.895 \u00b1 0.175\nhidden layers\n3\nbatch size\n256\noptimizer\nAdamx\nactivation\ntanh\nneurons\n256\nMALSTM-FCN\nepochs\n500\n0.73 \u00b1 0.027\n0.75 \u00b1 0.241\n0.611 \u00b1 0.175\n0.673 \u00b1 0.060\nhidden layers\n-\nbatch size\n256\noptimizer\nAdamx\nactivation\n-\nneurons\n-\nCNN\nepochs\n1000\n0.87 \u00b1 0.027\n0.782 \u00b1 0.175\n0.913 \u00b1 0.060\n0.842 \u00b1 0.228\nhidden layers\n2\nbatch size\n256\noptimizer\nNadam\nactivation\nrelu\nneurons\n256\nTable 21: Unrestricted model - Neural networks optimal parameters. We have also further explored the classi\ufb01cation via the unrestricted model at hourly frequency considering two sub-\nmodels: a sub-model including technical and social indicators and the other with all the indicators (social, technical and\ntrading).",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "5 Threats To Validity",
        "level": 1,
        "page_start": 26
      },
      "token_count": 379,
      "chunk_id": "chunk_53"
    },
    {
      "text": "We have also further explored the classi\ufb01cation via the unrestricted model at hourly frequency considering two sub-\nmodels: a sub-model including technical and social indicators and the other with all the indicators (social, technical and\ntrading). In this way, it is possible to disentangle impact of social and trading indicators on the models\u2019 performance. We used a statistical t test on the distributions of accuracy, prediction, recall and F1-score for the two unrestricted\nsub-modules \ufb01nding that adding social indicators does not add a signi\ufb01cant improvement to the unrestricted model. For this reason, in Table 22 we omitted the unrestricted model including social and technical indicators only. Table 23 shows the results obtained by the four deep learning algorithms for daily frequency price movements clas-\nsi\ufb01cation. This table presents results for both the restricted (upper part) and unrestricted (lower part) model. The\nunrestricted model is further divided in technical-social and techical-social-trading sub-models to better highlight the\ncontribution of social and trading indicators to the model separately. The MALSTM-CNF achieves the best classi\ufb01cation performance for Ethereum with 99% of accuracy using the re-\nstricted model composed of only technical indicators. For Bitcoin, the best results are achieved by MLP with F1-score\nof 55% and accuracy of 60% with the unrestricted model with only social media indicators and technical indicators\n(in this case, we consider F1-score and accuracy for Bitcoin because of the slightly unbalanced class distribution de-\nscribed in Section 2.4). For the daily frequency classi\ufb01cation, we can see that in general technical indicators alone\nperforms better in the classi\ufb01cation of next day price movement. The more indicators we add to the model, the more\nthe performance decrease.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "5 Threats To Validity",
        "level": 1,
        "page_start": 26
      },
      "token_count": 377,
      "chunk_id": "chunk_54"
    },
    {
      "text": "The more indicators we add to the model, the more\nthe performance decrease. Another general result is that the accuracy, precision, recall and F1-score for daily clas-\nsi\ufb01cation of Ethereum price movements are far better than those for Bitcoin. Results for daily classi\ufb01cation are in\nline with other studies [1] for the hourly and daily classi\ufb01cation with a signi\ufb01cant improvement when considering the\nhourly unrestricted model. The social media indicators turn out to be particularly relevant at the daily frequency for\nthe Bitcoin case. This result is in agreement with the recent result on the impact social media sentiment on cryptocur-\nrencies markets [2]: the effects of social media on markets show a long lag, which is not captured nor relevant at an\nhourly frequency.\n5\nThreats To Validity\nIn this section, we discuss potential limitations and threats to validity of our analysis.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "5 Threats To Validity",
        "level": 1,
        "page_start": 26
      },
      "token_count": 191,
      "chunk_id": "chunk_55"
    },
    {
      "text": "This result is in agreement with the recent result on the impact social media sentiment on cryptocur-\nrencies markets [2]: the effects of social media on markets show a long lag, which is not captured nor relevant at an\nhourly frequency.\n5\nThreats To Validity\nIn this section, we discuss potential limitations and threats to validity of our analysis. First, our analysis focuses\non Ethereum and Bitcoin: this may constitute a threat to external validity as conducting the analysis for different\ncryptocurrencies may lead to different results.\n26\nA PREPRINT - FEBRUARY 18, 2021\nModel\nAlgotithm\nCryptocurrency\nClass\nAccuracy\nPrecision\nRecall\nF1-score\nRestricted\nMLP\nbitcoin\ndown\n0.57\n0.28\n0.38\nup\n0.54\n0.80\n0.64\naverage\n0.55\n0.56\n0.55\n0.51\nethereum\ndown\n0.52\n0.77\n0.62\nup\n0.55\n0.29\n0.38\naverage\n0.53\n0.54\n0.53\n0.50\nMALSTM-FNC\nbitcoin\ndown\n0.52\n0.50\n0.51\nup\n0.55\n0.57\n0.56\naverage\n0.54\n0.54\n0.54\n0.54\nethereum\ndown\n0.52\n0.80\n0.63\nup\n0.57\n0.26\n0.36\naverage\n0.53\n0.54\n0.53\n0.50\nLSTM\nbitcoin\ndown\n0.49\n0.29\n0.37\nup\n0.53\n0.73\n0.61\naverage\n0.52\n0.51\n0.52\n0.49\nethereum\ndown\n0.51\n0.70\n0.59\nup\n0.51\n0.31\n0.39\naverage\n0.51\n0.51\n0.51\n0.49\nCNN\nbitcoin\ndown\n0.52\n0.65\n0.57\nup\n0.56\n0.42\n0.48\naverage\n0.53\n0.54\n0.53\n0.53\nethereum\ndown\n0.50\n0.75\n0.60\nup\n0.56\n0.31\n0.40\naverage\n0.52\n0.53\n0.52\n0.49\nUnrestricted\nMLP\nbitcoin\ndown\n0.87\n0.57\n0.69\nup\n0.70\n0.92\n0.79\naverage\n0.75\n0.78\n0.75\n0.74\nethereum\ndown\n0.80\n0.79\n0.80\nup\n0.80\n0.80\n0.80\naverage\n0.80\n0.80\n0.80\n0.80\nMALSTM-FNC\nbitcoin\ndown\n0.97\n0.32\n0.48\nup\n0.61\n0.99\n0.75\naverage\n0.67\n0.78\n0.67\n0.62\nethereum\ndown\n0.98\n0.15\n0.27\nup\n0.54\n1.00\n0.70\naverage\n0.58\n0.76\n0.58\n0.49\nLSTM\nbitcoin\ndown\n0.79\n0.90\n0.84\nup\n0.88\n0.76\n0.82\naverage\n0.83\n0.84\n0.83\n0.83\nethereum\ndown\n0.79\n0.91\n0.84\nup\n0.90\n0.76\n0.83\naverage\n0.84\n0.84\n0.84\n0.83\nCNN\nbitcoin\ndown\n0.82\n0.87\n0.84\nup\n0.87\n0.82\n0.85\naverage\n0.84\n0.84\n0.84\n0.84\nethereum\ndown\n0.72\n0.97\n0.83\nup\n0.95\n0.61\n0.74\naverage\n0.79\n0.83\n0.79\n0.78\nTable 22: Accuracy, Precision, Recall, F1 score for Restricted and Unrestricted models for each Deep Learning Algo-\nrithm With Hourly Frequency.\n27\nA PREPRINT - FEBRUARY 18, 2021\nModel\nFeatures\nAlgotithm\nCryptocurrency\nClass\nAccuracy\nPrecision\nRecall\nF1-score\ndown\n0.00\n0.00\n0.00\nup\n0.59\n0.96\n0.73\nbitcoin\naverage\n0.58\n0.36\n0.58\n0.44\ndown\n0.96\n1.00\n0.98\nup\n1.00\n0.96\n0.98\nMLP\nethereum\naverage\n0.98\n0.98\n0.98\n0.98\ndown\n0.51\n0.47\n0.49\nup\n0.56\n0.59\n0.58\nbitcoin\naverage\n0.54\n0.54\n0.54\n0.54\ndown\n1.00\n0.99\n0.99\nup\n0.99\n1.00\n0.99\nMALSTM-FNC\nethereum\naverage\n0.99\n0.99\n0.99\n0.99\ndown\n0.00\n0.00\n0.00\nup\n0.57\n1.00\n0.73\nbitcoin\naverage\n0.57\n0.33\n0.57\n0.41\ndown\n0.98\n0.98\n0.98\nup\n0.99\n0.99\n0.99\nLSTM\nethereum\naverage\n0.99\n0.99\n0.99\n0.99\ndown\n0.38\n0.10\n0.16\nup\n0.60\n0.89\n0.72\nbitcoin\naverage\n0.58\n0.51\n0.58\n0.50\ndown\n0.88\n1.00\n0.94\nup\n1.00\n0.88\n0.94\nRestricted\ntechnical\nCNN\nethereum\naverage\n0.94\n0.94\n0.94\n0.94\ndown\n0.59\n0.21\n0.31\nup\n0.61\n0.90\n0.72\nbitcoin\naverage\n0.60\n0.60\n0.60\n0.55\ndown\n0.79\n0.95\n0.87\nup\n0.95\n0.79\n0.87\nMLP\nethereum\naverage\n0.87\n0.88\n0.87\n0.87\ndown\n0.41\n0.41\n0.41\nup\n0.51\n0.51\n0.51\nbitcoin\naverage\n0.46\n0.46\n0.46\n0.46\ndown\n0.72\n0.70\n0.71\nup\n0.77\n0.78\n0.77\nMALSTM-FNC\nethereum\naverage\n0.75\n0.75\n0.75\n0.75\ndown\n0.44\n0.10\n0.17\nup\n0.47\n0.86\n0.60\nbitcoin\naverage\n0.46\n0.45\n0.46\n0.38\ndown\n0.88\n0.83\n0.85\nup\n0.87\n0.91\n0.89\nLSTM\nethereum\naverage\n0.87\n0.87\n0.87\n0.87\ndown\n0.42\n0.47\n0.44\nup\n0.56\n0.52\n0.54\nbitcoin\naverage\n0.50\n0.50\n0.50\n0.50\ndown\n0.77\n0.83\n0.80\nup\n0.85\n0.79\n0.82\ntechnical +\nsocial\nCNN\nethereum\naverage\n0.81\n0.81\n0.81\n0.81\ndown\n0.59\n0.20\n0.30\nup\n0.47\n0.84\n0.60\nbitcoin\naverage\n0.49\n0.54\n0.49\n0.43\ndown\n0.84\n0.91\n0.87\nup\n0.91\n0.84\n0.87\nMLP\nethereum\naverage\n0.87\n0.88\n0.87\n0.87\ndown\n0.41\n0.41\n0.41\nup\n0.62\n0.62\n0.62\nbitcoin\naverage\n0.54\n0.54\n0.54\n0.54\ndown\n0.79\n0.88\n0.83\nup\n0.91\n0.83\n0.87\nMALSTM-FNC\nethereum\naverage\n0.85\n0.86\n0.85\n0.85\ndown\n0.44\n0.31\n0.36\nup\n0.43\n0.58\n0.49\nbitcoin\naverage\n0.44\n0.44\n0.44\n0.43\ndown\n0.92\n0.87\n0.89\nup\n0.86\n0.91\n0.88\nLSTM\nethereum\naverage\n0.89\n0.89\n0.89\n0.89\ndown\n0.52\n0.55\n0.54\nup\n0.62\n0.59\n0.60\nbitcoin\naverage\n0.57\n0.57\n0.57\n0.57\ndown\n0.92\n0.85\n0.89\nup\n0.87\n0.93\n0.90\nUnrestricted\ntechnical +\nsocial +\ntrading\nCNN\nethereum\naverage\n0.89\n0.90\n0.89\n0.89\nTable 23: Accuracy, Precision, Recall, F1 score for Restricted and Unrestricted models for each Deep Learning Algo-\nrithm For Daily Frequency.\n28",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "5 Threats To Validity",
        "level": 1,
        "page_start": 26
      },
      "token_count": 2268,
      "chunk_id": "chunk_56"
    },
    {
      "text": "A PREPRINT - FEBRUARY 18, 2021\nSecondly, threats to internal validity concern confounding factors that can in\ufb02uence the obtained results. Based on the\nempirical evidence, we assume that technical, trading and social indicators are exhaustive in the case of our model. There may exists nonetheless other factors omitted from this study, which could in\ufb02uence the price movements. Finally, threats to construct validity focus on how accurately the observations describe the phenomena of interest. The\ndetection and classi\ufb01cation of price movements are based on objective data that describe the whole phenomenon. In\ngeneral, technical and trading indicators are based on objective data and are usually reliable. Social media indicators\nare based on empirical measures obtained via deep learning algorithms trained with publicly available datasets: these\ndatasets may carry intrinsic bias, which are in turn translated into classi\ufb01cation errors of emotion and sentiment.\n6\nConclusions\nSeveral attempts have been made in the most recent literature to model and predict the erratic behaviour of prices\nor other market indicators of the major cryptocurrencies. Notwithstanding massive efforts devoted to this goal by\nmany research groups, the analysis of cryptocurrency markets still remains one of the most debated and elusive tasks. Several aspects make grappling with this issue so complicated. For instance, due to its relatively young age, the\ncryptocurrency market is very dynamic and fast-paced. The emergence of new cryptocurrencies is a routine event,\nresulting in unexpected and frequent changes in the makeup of the market itself. Moreover, the high price volatility\nof cryptocurrencies and their \u2018virtual\u2019 nature are at the same time a blessing for investors and traders, and a curse for\nany serious theoretical and empirical modelling, with huge practical implications.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 357,
      "chunk_id": "chunk_57"
    },
    {
      "text": "Moreover, the high price volatility\nof cryptocurrencies and their \u2018virtual\u2019 nature are at the same time a blessing for investors and traders, and a curse for\nany serious theoretical and empirical modelling, with huge practical implications. The study of such a young market,\nwhose price behaviour is still largely unexplored, has fundamental repercussions not only in the scienti\ufb01c arena but\nalso for investors and main players and stakeholders in the crypto-market landscape. In this paper, we aimed to assess whether the addition of social and trading indicators to the \u201cclassic\u201d technical vari-\nables would lead to practical improvements in the classi\ufb01cation of price changes of cryptocurrencies considering\nhourly and daily frequencies. This goal was achieved implementing and benchmarking a wide array of deep learning\ntechniques, such as Multi-Layer Perceptron (MLP), Multivariate Attention Long Short Term Memory Fully Convo-\nlutional Network (MALSTM-FCN), Convolutional Neural Network (CNN) and Long Short Term Memory (LTMS)\nneural networks. We considered in our analysis the two main cryptocurrencies, Bitcoin and Ethereum, and we anal-\nysed two models: a restricted model, considering only technical indicators, and an unrestricted model that includes\nsocial and trading indicators. In the restricted analysis, the model that achieved the best performance, in terms of accuracy, precision, recall, and\nf1-score, is MALSTM-FCN with an average f1-score of 54% for Bitcoin and the CNN for Ethereum with hourly\nfrequency. For the unrestricted case the best result is achieved by the LSTM neural network for both Bitcoin and\nEthereum with an average accuracy of 83% and 84% respectively.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 350,
      "chunk_id": "chunk_58"
    },
    {
      "text": "For the unrestricted case the best result is achieved by the LSTM neural network for both Bitcoin and\nEthereum with an average accuracy of 83% and 84% respectively. The most important \ufb01nding for the hourly frequency\nclassi\ufb01cation for the unrestricted model is that the addition of trading and social indicators to the model leads to an\neffective improvement in the average accuracy, precision, recall, and f1-score. We have veri\ufb01ed that this \ufb01nding is\nnot the result of a statistical \ufb02uctuation, since all the implemented models yielded the same achievements. For the\nsame reason, we can exclude that the results depend on the particular implemented algorithm. Finally, for the daily\nclassi\ufb01cation, the best classi\ufb01cation performance has been achieved by MALSTM-CNF for Ethereum with 99% of\naccuracy when using the restricted model including only technical indicators. For Bitcoin, the best results are achieved\nby MLP with f1-score of 55% and accuracy of 60% with the unrestricted model including social media indicators and\ntechnical indicators, in this case, we consider f1-score and accuracy for Bitcoin because of the slightly unbalanced\nclass distribution described in Section 3.4. For the daily frequency classi\ufb01cation, we can see that in general technical\nindicators alone perform better in the classi\ufb01cation of next day price movements. The more indicators we add to the\nmodel, the more the performance decreases. Another general result is that the accuracy, precision, recall, and f1-score for daily classi\ufb01cation of Ethereum price\nmovements are far better than those for Bitcoin. Our results show that with a speci\ufb01c design and \ufb01ne-tuning of deep\nlearning architecture, it is possible to achieve high performance in the classi\ufb01cation of price changes of cryptocurren-\ncies.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 397,
      "chunk_id": "chunk_59"
    },
    {
      "text": "Our results show that with a speci\ufb01c design and \ufb01ne-tuning of deep\nlearning architecture, it is possible to achieve high performance in the classi\ufb01cation of price changes of cryptocurren-\ncies. References\n[1] Akyildirim, E., Goncu, A., Sensoy, A.: Prediction of cryptocurrency returns using machine learning. Annals of\nOperations Research pp. 1\u201334 (2020)\n[2] Bartolucci, S., Destefanis, G., Ortu, M., Uras, N., Marchesi, M., Tonelli, R.: The butter\ufb02y \u201caffect\u201d: Impact of\ndevelopment practices on cryptocurrency prices. EPJ Data Science 9(1), 21 (2020)\n29\nA PREPRINT - FEBRUARY 18, 2021\n[3] Calefato, F., Lanubile, F., Maiorano, F., Novielli, N.: Sentiment polarity detection for software development. Empirical Software Engineering pp. 1\u201331 (2017)\n[4] Calefato, F., Lanubile, F., Maiorano, F., Novielli, N.: Sentiment polarity detection for software development. In:\n2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). pp. 128\u2013128. IEEE (2018)\n[5] Chen, C.Y.H., Hafner, C.M.: Sentiment-induced bubbles in the cryptocurrency market. Journal of Risk and\nFinancial Management 12(2), 53 (2019)\n[6] Chen, J.F., Chen, W.L., Huang, C.P., Huang, S.H., Chen, A.P.: Financial time-series data analysis using deep\nconvolutional neural networks.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 385,
      "chunk_id": "chunk_60"
    },
    {
      "text": "Journal of Risk and\nFinancial Management 12(2), 53 (2019)\n[6] Chen, J.F., Chen, W.L., Huang, C.P., Huang, S.H., Chen, A.P.: Financial time-series data analysis using deep\nconvolutional neural networks. In: 2016 7th International conference on cloud computing and big data (CCBD).\npp. 87\u201392. IEEE (2016)\n[7] Chollet, F., et al.: Keras: The python deep learning library. Astrophysics Source Code Library pp. ascl\u20131806\n(2018)\n[8] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805 (2018)\n[9] Efron, B., Tibshirani, R.: The bootstrap method for assessing statistical accuracy. Behaviormetrika 12(17), 1\u201335\n(1985)\n[10] Hartmann, F., Grottolo, G., Wang, X., Lunesu, M.I.: Alternative fundraising: success factors for blockchain-\nbased vs. conventional crowdfunding. In: 2019 IEEE international workshop on blockchain oriented software\nengineering (IWBOSE). pp. 38\u201343. IEEE (2019)\n[11] Hartmann, F., Wang, X., Lunesu, M.I.: Evaluation of initial cryptoasset offerings: the state of the practice. In:\n2018 International Workshop on Blockchain Oriented Software Engineering (IWBOSE). pp. 33\u201339. IEEE (2018)\n[12] Hochreiter, S., Schmidhuber, J.: Long short-term memory.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 378,
      "chunk_id": "chunk_61"
    },
    {
      "text": "IEEE (2018)\n[12] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735\u20131780 (1997)\n[13] Jing-Zhi H., William H., J.N.:\nPredicting bitcoin returns using high-dimensional technical indicators. The Journal of Finance and Data Science (2018). https://doi.org/https://doi.org/10.1016/j.jfds.2018.10.001,\nhttp://www.sciencedirect.com/science/article/pii/S2405918818300928\n[14] Karim, F., Majumdar, S., Darabi, H., Chen, S.: Lstm fully convolutional networks for time series classi\ufb01cation. IEEE access 6, 1662\u20131669 (2017)\n[15] Karim, F., Majumdar, S., Darabi, H., Harford, S.: Multivariate lstm-fcns for time series classi\ufb01cation. Neural\nNetworks 116, 237\u2013245 (2019)\n[16] Katsiampa, P.: Volatility estimation for bitcoin: A comparison of garch models. Economics Letters 158, 3\u20136\n(2017)\n[17] Lahmiri, S., B.S.: Cryptocurrency forecasting with deep learning chaotic neural networks. Chaos, Solitons and\nFractals 118, 35 \u2013 40 (2019)\n[18] Lahmiri, S., Bekiros, S., Salvi, A.: Long-range memory, distributional variation and randomness of bitcoin\nvolatility. Chaos, Solitons & Fractals 107, 43\u201348 (2018)\n[19] Lerman, P.: Fitting segmented regression models by grid search.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 389,
      "chunk_id": "chunk_62"
    },
    {
      "text": "Chaos, Solitons & Fractals 107, 43\u201348 (2018)\n[19] Lerman, P.: Fitting segmented regression models by grid search. Journal of the Royal Statistical Society: Series\nC (Applied Statistics) 29(1), 77\u201384 (1980)\n[20] M\u00a8antyl\u00a8a, M., Adams, B., Destefanis, G., Graziotin, D., Ortu, M.: Mining valence, arousal, and dominance:\npossibilities for detecting burnout and productivity? In: Proceedings of the 13th international conference on\nmining software repositories. pp. 247\u2013258 (2016)\n[21] Marchesi, L., Marchesi, M., Destefanis, G., Barabino, G., Tigano, D.: Design patterns for gas optimization in\nethereum. In: 2020 IEEE International Workshop on Blockchain Oriented Software Engineering (IWBOSE). pp.\n9\u201315. IEEE (2020)\n[22] Matta, M., Lunesu, I., Marchesi, M.: Bitcoin spread prediction using social and web search media. In: UMAP\nworkshops. pp. 1\u201310 (2015)\n[23] Murgia, A., Ortu, M., Tourani, P., Adams, B., Demeyer, S.: An exploratory qualitative and quantitative analysis\nof emotions in issue report comments of open source systems. Empirical Software Engineering 23(1), 521\u2013564\n(2018)\n[24] Murgia, A., Tourani, P., Adams, B., Ortu, M.: Do developers feel emotions? an exploratory analysis of emotions\nin software artifacts. In: Proceedings of the 11th Working Conference on Mining Software Repositories. pp.\n262\u2013271.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 387,
      "chunk_id": "chunk_63"
    },
    {
      "text": "In: Proceedings of the 11th Working Conference on Mining Software Repositories. pp.\n262\u2013271. ACM (2014)\n[25] Ortu, M.: Mining software repositories: measuring effectiveness and affectiveness in software systems. (2015)\n30\nA PREPRINT - FEBRUARY 18, 2021\n[26] Ortu, M., Hall, T., Marchesi, M., Tonelli, R., Bowes, D., Destefanis, G.: Mining communication patterns in\nsoftware development: A github analysis. In: Proceedings of the 14th International Conference on Predictive\nModels and Data Analytics in Software Engineering. pp. 70\u201379. ACM (2018)\n[27] Ortu, M., Orr\u00b4u, M., Destefanis, G.: On comparing software quality metrics of traditional vs blockchain-oriented\nsoftware: An empirical study. In: 2019 IEEE International Workshop on Blockchain Oriented Software Engi-\nneering (IWBOSE). pp. 32\u201337. IEEE (2019)\n[28] Phillips, R.C., Gorse, D.: Mutual-excitation of cryptocurrency market returns and social media topics. In: Pro-\nceedings of the 4th International Conference on Frontiers of Educational Technologies. pp. 80\u201386. ACM (2018)\n[29] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention\nis all you need. arXiv preprint arXiv:1706.03762 (2017)\n[30] Warriner, A.B., Kuperman, V., Brysbaert, M.: Norms of valence, arousal, and dominance for 13,915 english\nlemmas.",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 396,
      "chunk_id": "chunk_64"
    },
    {
      "text": "ACM (2018)\n[29] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention\nis all you need. arXiv preprint arXiv:1706.03762 (2017)\n[30] Warriner, A.B., Kuperman, V., Brysbaert, M.: Norms of valence, arousal, and dominance for 13,915 english\nlemmas. Behavior Research Methods 45(4), 1191\u20131207 (dec 2013). https://doi.org/10.3758/s13428-012-0314-x\n31",
      "metadata": {
        "paper_title": "arXiv:2102.08189v2  [q-fin.ST]  17 Feb 2021",
        "section": "6 Conclusions",
        "level": 1,
        "page_start": 29
      },
      "token_count": 161,
      "chunk_id": "chunk_65"
    }
  ],
  "stats": {
    "num_chunks": 66
  }
}